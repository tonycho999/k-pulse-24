import os
import json
import urllib.parse
import urllib.request
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
import feedparser 

def get_naver_api_news(keyword):
    """ë„¤ì´ë²„ API ë‰´ìŠ¤ ê²€ìƒ‰ (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)"""
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=date"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    
    try:
        print(f"ğŸ“¡ ë„¤ì´ë²„ API í˜¸ì¶œ ì¤‘: {keyword}...")
        res = urllib.request.urlopen(req, timeout=10) 
        items = json.loads(res.read().decode('utf-8')).get('items', [])
        
        valid_items = []
        now = datetime.now()
        threshold = now - timedelta(hours=24)

        for item in items:
            try:
                pub_date = parsedate_to_datetime(item['pubDate']).replace(tzinfo=None)
                if pub_date < threshold:
                    continue
                item['published_at'] = pub_date
                valid_items.append(item)
            except:
                continue

        return valid_items

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ API ì—ëŸ¬ ({keyword}): {e}")
        return []

def get_article_data(link):
    """
    [ì—…ê·¸ë ˆì´ë“œ] ê¸°ì‚¬ ë³¸ë¬¸(1,500ì) ë° ì´ë¯¸ì§€ í†µí•© ì¶”ì¶œ í•¨ìˆ˜
    * ìˆ˜ì •ì‚¬í•­: Mixed Content ë°©ì§€ë¥¼ ìœ„í•´ HTTPS ì´ë¯¸ì§€ ê°•ì œ
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        # íƒ€ì„ì•„ì›ƒ 5ì´ˆ ì„¤ì •
        res = requests.get(link, headers=headers, timeout=5)
        
        if res.status_code != 200:
            return "", None

        soup = BeautifulSoup(res.text, 'html.parser')
        
        # --- 1. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¬ë£Œ í™•ë³´) ---
        # ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë“¤ì˜ ë³¸ë¬¸ ì˜ì—­ íƒœê·¸ ëª¨ìŒ
        content_area = soup.select_one('#dic_area, #articleBodyContents, .article_view, #articeBody, .news_view, #newsct_article, .article-body')
        
        full_text = ""
        if content_area:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³  ë²„íŠ¼ ë“±)
            for s in content_area(['script', 'style', 'iframe', 'button', 'a', 'div.ad']):
                s.decompose()
            full_text = content_area.get_text(separator=' ', strip=True)
            # [í•µì‹¬] ìš”ì•½ í’ˆì§ˆì„ ìœ„í•´ ìµœëŒ€ 1,500ìê¹Œì§€ í™•ë³´
            full_text = full_text[:1500]
        else:
            # ë³¸ë¬¸ íƒœê·¸ë¥¼ ëª» ì°¾ì•˜ì„ ê²½ìš°, body ì „ì²´ì—ì„œ í…ìŠ¤íŠ¸ë§Œì´ë¼ë„ ê¸ì–´ì˜¤ê¸° ì‹œë„ (ìµœí›„ì˜ ìˆ˜ë‹¨)
            full_text = soup.body.get_text(separator=' ', strip=True)[:1000] if soup.body else ""

        # --- 2. ì´ë¯¸ì§€ ì¶”ì¶œ (HTTPS ê°•ì œ ì ìš©) ---
        image_url = None
        
        # ë³¸ë¬¸ ì˜ì—­ ì•ˆì˜ ì´ë¯¸ì§€ë¥¼ 1ìˆœìœ„ë¡œ ì°¾ìŒ
        if content_area:
            imgs = content_area.find_all('img')
            for i in imgs:
                src = i.get('src') or i.get('data-src')
                
                # [ìˆ˜ì •] http:// ëŠ” ë²„ë¦¬ê³  ë°˜ë“œì‹œ https:// ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë§Œ ê°€ì ¸ì˜´
                if src and src.startswith('https://'):
                    # ë„ˆë¬´ ì‘ì€ ì•„ì´ì½˜/ë°°ë„ˆ ì œì™¸
                    width = i.get('width')
                    if width and width.isdigit() and int(width) < 200: continue
                    image_url = src
                    break

        # ë³¸ë¬¸ì— ì—†ìœ¼ë©´ ë©”íƒ€ íƒœê·¸(og:image) í™•ì¸
        if not image_url:
            og = soup.find('meta', property='og:image')
            if og and og.get('content'): 
                candidate = og['content']
                # [ìˆ˜ì •] ë©”íƒ€ íƒœê·¸ ì´ë¯¸ì§€ë„ https ì¸ì§€ í™•ì¸
                if candidate.startswith('https://'):
                    image_url = candidate

        # ë¶ˆëŸ‰ ì´ë¯¸ì§€ í‚¤ì›Œë“œ í•„í„°ë§
        if image_url:
            bad_keywords = r'logo|icon|button|share|banner|thumb|profile|default|ranking|news_stand|ssl.pstatic.net'
            if re.search(bad_keywords, image_url, re.IGNORECASE): 
                image_url = None

        return full_text, image_url

    except Exception as e:
        # print(f"    âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({link[:30]}...): {e}")
        return "", None

def get_google_trending_keywords():
    """(ë¯¸ë˜ ì‚¬ìš©ìš©) êµ¬ê¸€ íŠ¸ë Œë“œ RSS ìˆ˜ì§‘"""
    try:
        url = "https://trends.google.com/trends/trendingsearches/daily/rss?geo=KR"
        feed = feedparser.parse(url)
        keywords = [entry.title for entry in feed.entries]
        return keywords
    except Exception as e:
        # print(f"âŒ êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []
