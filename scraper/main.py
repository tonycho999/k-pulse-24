import os
import sys
import json
import urllib.request
import urllib.parse
import requests  # ì¶”ê°€ë¨
from bs4 import BeautifulSoup  # ì¶”ê°€ë¨
from supabase import create_client, Client
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
supabase_key = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
groq_api_key = os.environ.get("GROQ_API_KEY")
naver_client_id = os.environ.get("NAVER_CLIENT_ID")
naver_client_secret = os.environ.get("NAVER_CLIENT_SECRET")

if not all([supabase_url, supabase_key, groq_api_key, naver_client_id, naver_client_secret]):
    print("âŒ Error: .env í‚¤ í™•ì¸ í•„ìš”")
    sys.exit(1)

supabase: Client = create_client(supabase_url, supabase_key)
groq_client = Groq(api_key=groq_api_key)
AI_MODEL = "llama-3.3-70b-versatile"

# ê²€ìƒ‰í•  í‚¤ì›Œë“œ
SEARCH_KEYWORDS = ["K-POP ì•„ì´ëŒ", "í•œêµ­ ì¸ê¸° ë“œë¼ë§ˆ", "í•œêµ­ ì˜í™” í™”ì œ", "í•œêµ­ ì˜ˆëŠ¥ ë ˆì „ë“œ"]

def get_real_news_image(link):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ì— ì ‘ì†í•´ì„œ ì‹¤ì œ og:image(ëŒ€í‘œ ì´ë¯¸ì§€) ì£¼ì†Œë¥¼ ê°€ì ¸ì˜´
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(link, headers=headers, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # og:image ë©”íƒ€ íƒœê·¸ ì°¾ê¸°
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                return og_image['content']
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {link} -> {e}")
    return None

def get_naver_api_news(keyword):
    encText = urllib.parse.quote(keyword)
    url = f"https://openapi.naver.com/v1/search/news?query={encText}&display=15&sort=sim"
    
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", naver_client_id)
    request.add_header("X-Naver-Client-Secret", naver_client_secret)
    
    try:
        response = urllib.request.urlopen(request)
        if response.getcode() == 200:
            data = json.loads(response.read().decode('utf-8'))
            return data.get('items', [])
    except Exception as e:
        print(f"API Error: {e}")
    return []

def ai_chief_editor(news_batch):
    news_text = ""
    for idx, item in enumerate(news_batch):
        clean_title = item['title'].replace('<b>', '').replace('</b>', '').replace('&quot;', '"')
        news_text += f"{idx+1}. {clean_title}\n"

    prompt = f"""
    Role: Chief Editor of 'K-ENTER 24'.
    Task:
    Analyze news and select Top 12.
    Output JSON strictly:
    {{
        "global_insight": "Summary...",
        "articles": [
            {{
                "category": "K-POP", 
                "artist": "Subject",
                "title": "English Headline",
                "summary": "Short summary",
                "score": 9,
                "reactions": {{"excitement": 80, "sadness": 0, "shock": 20}},
                "original_title_index": 1 
            }}
        ]
    }}
    Raw Titles:
    {news_text}
    """
    try:
        res = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=AI_MODEL,
            response_format={"type": "json_object"}
        )
        return json.loads(res.choices[0].message.content)
    except Exception as e:
        print(f"AI Editor Error: {e}")
        return None

def run():
    print(f"=== {datetime.now()} K-Enter 24 ì‹¤ì „ ëª¨ë“œ ê°€ë™ ===")
    
    all_news = []
    for keyword in SEARCH_KEYWORDS:
        print(f"ğŸ“¡ ìˆ˜ì§‘ ì¤‘: {keyword}")
        all_news.extend(get_naver_api_news(keyword))
    
    if not all_news: return

    print("ğŸ“ AI ë¶„ì„ ë° ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘...")
    result = ai_chief_editor(all_news)
    if not result: return

    saved_count = 0
    for article in result.get('articles', []):
        idx = article.get('original_title_index', 1) - 1
        if idx < 0 or idx >= len(all_news): idx = 0
        original = all_news[idx]

        # --- [í•µì‹¬] ì‹¤ì œ ê¸°ì‚¬ ë§í¬ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ---
        real_img = get_real_news_image(original['link'])
        
        # ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ì„ ë•Œë§Œ ë³´ì¡° ì´ë¯¸ì§€ ì‚¬ìš©
        if not real_img:
            subject = article.get('artist', 'News')
            real_img = f"https://placehold.co/600x400/111/cyan?text={subject.replace(' ', '+')}"

        try:
            # ì¤‘ë³µ ì²´í¬
            if supabase.table("live_news").select("id").eq("title", article['title']).execute().data:
                continue
            
            data = {
                "category": article.get('category', 'General'),
                "artist": article.get('artist', 'Trend'),
                "title": article['title'],
                "summary": article['summary'],
                "score": article.get('score', 5),
                "link": original['link'],
                "source": "Naver News",
                "image_url": real_img,  # ì‹¤ì œ ì´ë¯¸ì§€ ì£¼ì†Œ ì €ì¥
                "reactions": article['reactions'],
                "is_published": True,
                "created_at": datetime.now().isoformat()
            }
            supabase.table("live_news").insert(data).execute()
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {article['title']}")
            saved_count += 1
        except Exception as e:
            print(f"ì €ì¥ ì‹¤íŒ¨: {e}")

    print(f"=== ì™„ë£Œ: {saved_count}ê°œì˜ ì§„ì§œ ë‰´ìŠ¤ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤ ===")

if __name__ == "__main__":
    run()
