import sys
import os

# ëª¨ë“ˆ import ë¬¸ì œ ë°©ì§€
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import time
from datetime import datetime
from dotenv import load_dotenv

# í•„ìˆ˜ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from scraper import crawler, ai_engine, repository, update_rankings

load_dotenv()

def run_master_scraper():
    print("ğŸš€ êµ¬ê¸€ íŠ¸ë Œë“œ ê¸°ë°˜ 9ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™...")
    
    # [1ë‹¨ê³„] êµ¬ê¸€ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘
    raw_trending_keywords = crawler.get_google_trending_keywords()
    if not raw_trending_keywords:
        print("âš ï¸ êµ¬ê¸€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨. ê³ ì • í‚¤ì›Œë“œë¡œ ëŒ€ì²´ ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    # [2~3ë‹¨ê³„] AI ë¶„ë¥˜ ë° ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì„ ì • (ì´ 50ê°œ ë‚´ì™¸)
    # Groq -> OpenRouter -> HF ìˆœìœ¼ë¡œ ì‹œë„í•˜ë©° í•„í„°ë§í•¨
    categorized_keywords = ai_engine.ai_filter_and_rank_keywords(raw_trending_keywords)
    
    if not categorized_keywords:
        print("âŒ AI í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹¤íŒ¨. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # ì¹´í…Œê³ ë¦¬ë³„ ë£¨í”„ ì‹œì‘
    for category, keywords in categorized_keywords.items():
        try:
            print(f"\nğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘ (í‚¤ì›Œë“œ: {keywords})")

            # [4ë‹¨ê³„] ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë³¸ë¬¸ í¬ë¡¤ë§
            raw_news = []
            for kw in keywords: 
                raw_news.extend(crawler.get_naver_api_news(kw))
            
            # [5ë‹¨ê³„] DB ì¤‘ë³µ ì²´í¬
            existing_links = repository.get_existing_links(category)
            new_candidate_news = [n for n in raw_news if n['link'] not in existing_links][:70]

            if not new_candidate_news:
                print(f"    âœ¨ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            # ë³¸ë¬¸ 1,500ì ë° ì´ë¯¸ì§€ í™•ë³´
            print(f"    ğŸ•·ï¸ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘ ({len(new_candidate_news)}ê°œ)...")
            for news_item in new_candidate_news:
                full_text, image_url = crawler.get_article_data(news_item['link'])
                news_item['full_content'] = full_text  
                news_item['crawled_image'] = image_url 

            # [6ë‹¨ê³„] 3ì¤‘ AI ì—”ì§„ì„ ì´ìš©í•œ í‰ì  ë° 3ë‹¨ê³„ ìš”ì•½
            analyzed_list = ai_engine.ai_category_editor(category, new_candidate_news)
            
            if analyzed_list:
                # ì ìˆ˜ ë†’ì€ ìˆœ ì •ë ¬ í›„ ìƒìœ„ 30ê°œ ì„ ì •
                analyzed_list.sort(key=lambda x: x.get('score', 0), reverse=True)
                top_30_news = analyzed_list[:30]
                
                # [7ë‹¨ê³„] DB ì €ì¥ (7.0ì  ì´ìƒ ì•„ì¹´ì´ë¹™ í¬í•¨)
                new_data_list = []
                for art in top_30_news:
                    idx = art.get('original_index')
                    if idx is not None and idx < len(new_candidate_news):
                        orig = new_candidate_news[idx]
                        new_data_list.append({
                            "category": category, 
                            "title": art.get('eng_title', orig['title']),
                            "summary": art.get('summary', 'Summary not available.'), 
                            "link": orig['link'], 
                            "image_url": orig.get('crawled_image') or f"https://placehold.co/600x400/111/cyan?text={category}",
                            "score": art.get('score', 5.0), 
                            "likes": 0, "dislikes": 0, 
                            "created_at": datetime.now().isoformat(),
                            "published_at": orig.get('published_at', datetime.now()).isoformat()
                        })
                repository.save_news(new_data_list)

            # [8~9ë‹¨ê³„] ìŠ¬ë¡¯ ê´€ë¦¬ (30ê°œ ìœ ì§€, ì‹œê°„/ì ìˆ˜ìˆœ ì‚­ì œ)
            repository.manage_slots(category)

        except Exception as e:
            print(f"âš ï¸ {category} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    print("\nğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì™„ë£Œ.")

def main():
    print("ğŸš€ K-Enter AI News Bot Master Mode Started...")
    # ìˆœìœ„ ì—…ë°ì´íŠ¸ ì‹¤í–‰
    try: update_rankings.update_rankings() 
    except: pass
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘
    run_master_scraper()

if __name__ == "__main__":
    main()
