import os
import json
import time
import requests
import urllib.parse
from supabase import create_client, Client
from dotenv import load_dotenv
from ddgs import DDGS

# 1. ÌôòÍ≤ΩÎ≥ÄÏàò Î°úÎìú
load_dotenv()

# 2. Supabase ÏÑ§Ï†ï
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 3. Gemini API ÌÇ§
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

if GOOGLE_API_KEY:
    print(f"üîë API Key Loaded: {GOOGLE_API_KEY[:5]}...")
else:
    print("‚ùå No API Key found!")

# ‚úÖ [ÌïµÏã¨ Î≥ÄÍ≤Ω 1] Í≤ÄÏÉâÏñ¥Î•º 'ÌïúÍµ≠Ïñ¥'Î°ú Î≥ÄÍ≤ΩÌï¥Ïïº ÏµúÏã† Îâ¥Ïä§Í∞Ä Ïû°Ìûò
CATEGORIES = {
    "K-Pop": "K-POP ÏïÑÏù¥Îèå ÏµúÏã† Îâ¥Ïä§ Ïª¥Î∞±",
    "K-Drama": "ÌïúÍµ≠ ÎìúÎùºÎßà ÏãúÏ≤≠Î•† ÏàúÏúÑ ÏµúÏã† Îâ¥Ïä§",
    "K-Movie": "ÌïúÍµ≠ ÏòÅÌôî Î∞ïÏä§Ïò§ÌîºÏä§ Í∞úÎ¥âÏûë Î∞òÏùë",
    "K-Entertain": "ÌïúÍµ≠ ÏòàÎä• ÌîÑÎ°úÍ∑∏Îû® ÏãúÏ≤≠Î•† ÌôîÏ†úÏÑ±", 
    "K-Culture": "ÏÑúÏö∏ Ìï´ÌîåÎ†àÏù¥Ïä§ Ïú†Ìñâ ÌåùÏóÖÏä§ÌÜ†Ïñ¥ Ìä∏Î†åÎìú" 
}

# Î™®Îç∏ ÏûêÎèô ÌÉêÏÉâ (404 Î∞©ÏßÄ)
def get_dynamic_model_url():
    print("üîç Fetching available Gemini models...")
    list_url = f"https://generativelanguage.googleapis.com/v1beta/models?key={GOOGLE_API_KEY}"
    
    try:
        response = requests.get(list_url)
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            valid_models = [
                m['name'] for m in models 
                if 'generateContent' in m.get('supportedGenerationMethods', []) 
                and 'flash' in m['name']
            ]
            if valid_models:
                best_model = valid_models[-1] 
                if not best_model.startswith("models/"):
                    best_model = f"models/{best_model}"
                print(f"‚úÖ Selected Model: {best_model}")
                return f"https://generativelanguage.googleapis.com/v1beta/{best_model}:generateContent"
    except Exception as e:
        print(f"‚ö†Ô∏è Model fetch failed: {e}")

    return "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"

CURRENT_MODEL_URL = get_dynamic_model_url()

def get_fallback_image(keyword):
    """Îâ¥Ïä§Ïóê Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏùÑ Îïå Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâ (ÌïúÍµ≠Ïñ¥ Í≤ÄÏÉâ)"""
    try:
        with DDGS() as ddgs:
            # Ïó¨Í∏∞ÎèÑ kr-krÎ°ú Í≤ÄÏÉâÌï¥Ïïº Ïù¥ÎØ∏ÏßÄÍ∞Ä Ïûò ÎÇòÏò¥
            imgs = list(ddgs.images(keywords=keyword, region="kr-kr", safesearch="off", max_results=1))
            if imgs and len(imgs) > 0:
                return imgs[0].get('image')
    except Exception:
        return ""
    return ""

def search_web(keyword):
    """
    DuckDuckGo Í≤ÄÏÉâ: 
    - ÌÇ§ÏõåÎìú: ÌïúÍµ≠Ïñ¥
    - ÏßÄÏó≠: ÌïúÍµ≠ (kr-kr) -> Ïù¥Í≤å ÌïµÏã¨!
    - Í∏∞Í∞Ñ: ÏßÄÎÇú 24ÏãúÍ∞Ñ (d)
    """
    print(f"üîç [Search] Searching for '{keyword}' in Korea (Last 24h)...")
    results = []
    
    try:
        with DDGS() as ddgs:
            # ‚úÖ [ÌïµÏã¨ Î≥ÄÍ≤Ω 2] region="kr-kr" (ÌïúÍµ≠)
            ddg_results = list(ddgs.news(
                query=keyword, 
                region="kr-kr",   # ÌïúÍµ≠ Îâ¥Ïä§Îßå Í≤ÄÏÉâ
                safesearch="off", 
                timelimit="d",    # ÏßÄÎÇú 24ÏãúÍ∞Ñ (ÌïúÍµ≠Ïñ¥Îùº Ïù¥Ï†ú Îç∞Ïù¥ÌÑ∞ ÎßéÏùå)
                max_results=15
            ))
            
            for r in ddg_results:
                title = r.get('title', '')
                body = r.get('body', r.get('snippet', ''))
                link = r.get('url', r.get('href', ''))
                image = r.get('image', r.get('thumbnail', ''))

                if not title or not body or not link or not link.startswith("https"):
                    continue

                if not image:
                    image = get_fallback_image(title)
                    time.sleep(0.3) 

                if not image:
                    continue

                results.append(f"Title: {title}\nBody: {body}\nLink: {link}\nImage: {image}")
                
    except Exception as e:
        print(f"‚ö†Ô∏è Search error: {e}")
    
    return "\n\n".join(results)

def call_gemini_api(category_name, raw_data):
    print(f"ü§ñ [Gemini] Translating & Writing '{category_name}' articles...")
    
    headers = {"Content-Type": "application/json"}
    
    # ‚úÖ [ÌïµÏã¨ Î≥ÄÍ≤Ω 3] ÌïúÍµ≠Ïñ¥ Îç∞Ïù¥ÌÑ∞Î•º Ï§Ñ ÌÖåÎãà -> ÏòÅÏñ¥Î°ú Í∏∞ÏÇ¨Î•º Ïç®Îùº (Î≤àÏó≠+ÏöîÏïΩ)
    prompt = f"""
    [Role]
    You are a veteran K-Entertainment journalist writing for an international audience.
    
    [Input Data (Korean News)]
    {raw_data[:25000]} 

    [Task]
    1. Read the Korean news provided above.
    2. Select the Top 10 most viral/important news items.
    3. **Rewrite/Translate them into PERFECT ENGLISH.**
    
    [Content Requirements - STRICT]
    1. **Language**: Output MUST be in **ENGLISH**.
    2. **Length**: 100~500 characters per summary.
    3. **Style**: Insightful, catchy, and professional.
    4. **Image**: Map the 'image_url' from raw data exactly.

    [Output Format (JSON Only)]
    {{
      "news_updates": [
        {{ 
          "keyword": "Main Subject (English)", 
          "title": "Title (English)", 
          "summary": "Summary (English, 100-500 chars)", 
          "link": "Original Link",
          "image_url": "URL starting with https"
        }}
      ],
      "rankings": [
        {{ "rank": 1, "title": "Name (English)", "meta": "Info (English)", "score": 98 }}
      ]
    }}
    """
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    full_url = f"{CURRENT_MODEL_URL}?key={GOOGLE_API_KEY}"

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.post(full_url, headers=headers, json=payload)
            
            if response.status_code == 200:
                try:
                    text = response.json()['candidates'][0]['content']['parts'][0]['text']
                    text = text.replace("```json", "").replace("```", "").strip()
                    return json.loads(text)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è JSON Parse Error: {e}")
                    return None
            
            elif response.status_code in [404]:
                 print(f"   ‚ùå Model Not Found (404).")
                 return None

            elif response.status_code in [429, 503]:
                wait_time = (attempt + 1) * 10
                print(f"   ‚ùå Temporary Error ({response.status_code}). Retrying...")
                time.sleep(wait_time)
                continue
            
            else:
                print(f"   ‚ùå API Error ({response.status_code}): {response.text[:200]}")
                return None

        except Exception as e:
            print(f"   ‚ùå Connection Error: {e}")
            time.sleep(5)
            continue
            
    return None

def update_database(category, data):
    news_list = data.get("news_updates", [])
    if news_list:
        clean_news = []
        for item in news_list:
            if not item.get("image_url"): continue

            summary = item.get("summary", "")
            title = item.get("title", "No Title")
            
            if len(summary) < 50: 
                continue

            # Íµ¨Í∏Ä Îâ¥Ïä§ Í≤ÄÏÉâ ÎßÅÌÅ¨ ÏÉùÏÑ±
            encoded_query = urllib.parse.quote(f"{title} k-pop news")
            search_link = f"https://www.google.com/search?q={encoded_query}&tbm=nws"

            clean_news.append({
                "category": category,
                "keyword": item.get("keyword", category),
                "title": title,
                "summary": summary,
                "link": search_link,
                "image_url": item.get("image_url"),
                "created_at": "now()",
                "likes": 0,
                "score": 80 + (len(summary) / 10) 
            })
        
        if clean_news:
            try:
                supabase.table("live_news").upsert(clean_news, on_conflict="category,keyword,title").execute()
                supabase.table("search_archive").upsert(clean_news, on_conflict="category,keyword,title").execute()
                print(f"   üíæ Saved {len(clean_news)} news items.")
            except Exception as e:
                print(f"   ‚ö†Ô∏è DB Save Error: {e}")

    rank_list = data.get("rankings", [])
    if rank_list:
        clean_ranks = []
        for item in rank_list:
            clean_ranks.append({
                "category": category,
                "rank": item.get("rank"),
                "title": item.get("title"),
                "meta_info": item.get("meta", ""),
                "score": item.get("score", 0),
                "updated_at": "now()"
            })
        try:
            supabase.table("live_rankings").upsert(clean_ranks, on_conflict="category,rank").execute()
            print(f"   üèÜ Updated rankings.")
        except Exception as e:
             print(f"   ‚ö†Ô∏è Ranking Save Error: {e}")

def main():
    print(f"üöÄ Scraper Started (Korea Region Source -> English Output)")
    for category, search_keyword in CATEGORIES.items():
        raw_text = search_web(search_keyword)
        
        if len(raw_text) < 50: 
            print(f"‚ö†Ô∏è {category} : Not enough data (Surprisingly).")
            continue

        data = call_gemini_api(category, raw_text)
        if data:
            update_database(category, data)
        
        print("‚è≥ Cooldown (5s)...")
        time.sleep(5) 

    print("‚úÖ All jobs finished.")

if __name__ == "__main__":
    main()
