import sys
import os
import time
from datetime import datetime, timedelta
from dateutil import parser # ë‚ ì§œ íŒŒì‹±ìš© (ì—†ìœ¼ë©´ pip install python-dateutil)
from dotenv import load_dotenv

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS

load_dotenv()

# [ì„¤ì •] ìƒìœ„ 30ê°œ ë¶„ì„
TARGET_RANK_LIMIT = 30 

# [ë„êµ¬] 24ì‹œê°„ ì´ë‚´ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
def is_within_24h(date_str):
    if not date_str: return False
    try:
        # ë„¤ì´ë²„ API ë‚ ì§œ í¬ë§· (Fri, 14 Feb 2026 10:00:00 +0900) ë“± ì²˜ë¦¬
        pub_date = parser.parse(date_str)
        # íƒ€ì„ì¡´ ì •ë³´ ì œê±° (ë‹¨ìˆœ ë¹„êµìš©)
        if pub_date.tzinfo:
            pub_date = pub_date.replace(tzinfo=None)
            
        now = datetime.now()
        diff = now - pub_date
        return diff <= timedelta(hours=24)
    except:
        return False # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ì œì™¸

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # ==========================================
        # [1ë‹¨ê³„] ì”¨ì•— ë°ì´í„° ìˆ˜ì§‘ (24ì‹œê°„ ì´ë‚´)
        # ==========================================
        raw_text_data = [] # ì œëª© + ìš”ì•½ë³¸
        
        try:
            for seed in seeds:
                # 100ê°œ ì •ë„ ë„‰ë„‰íˆ ê°€ì ¸ì™€ì„œ ë‚ ì§œë¡œ ìë¦„
                news_items = crawler.get_naver_api_news(seed, display=50)
                
                for item in news_items:
                    # 24ì‹œê°„ í•„í„°ë§
                    if is_within_24h(item.get('pubDate')):
                        # ì œëª©ê³¼ ìš”ì•½ë³¸ì„ í•©ì³ì„œ ë¶„ì„ ë°ì´í„°ë¡œ ì‚¬ìš©
                        combined_text = f"Title: {item['title']}\nSummary: {item['description']}"
                        raw_text_data.append(combined_text)
                        
            print(f"   ğŸŒ± 24ì‹œê°„ ë‚´ ìœ íš¨ ê¸°ì‚¬ ìˆ˜ì§‘: {len(raw_text_data)}ê°œ")
            
            if len(raw_text_data) < 5:
                print("   âš ï¸ ê¸°ì‚¬ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue
                
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            continue
        
        # ==========================================
        # [2ë‹¨ê³„] AI í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì •ì²´ ë¶„ë¥˜
        # ==========================================
        # top_entities = [{'keyword': 'Hype Boy', 'type': 'content'}, ...]
        top_entities = ai_engine.extract_top_entities(category, "\n".join(raw_text_data))
        
        if not top_entities: 
            print("   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
            continue
            
        print(f"   ğŸ’ ì¶”ì¶œëœ í‚¤ì›Œë“œ (Top 5): {', '.join([e['keyword'] for e in top_entities[:5]])}...")

        # ==========================================
        # [3ë‹¨ê³„] ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ ì‹¬ì¸µ ë¶„ì„
        # ==========================================
        category_news_list = []
        target_list = top_entities[:TARGET_RANK_LIMIT]
        
        for rank, entity in enumerate(target_list):
            kw = entity.get('keyword')
            k_type = entity.get('type', 'content') # ê¸°ë³¸ê°’ content
            
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ({k_type}) ë¶„ì„ ì¤‘...")
            
            try:
                # 1. ê²€ìƒ‰ (ìµœì‹ ìˆœ sort='date' ê¶Œì¥í•˜ì§€ë§Œ ì •í™•ë„ ìœ„í•´ sim í›„ ë‚ ì§œ í•„í„°ë§)
                raw_articles = crawler.get_naver_api_news(kw, display=30)
                if not raw_articles: continue

                full_contents = []
                main_image = None
                
                # 2. ê¸°ì‚¬ ìˆœíšŒ (24ì‹œê°„ ì´ë‚´ + í‚¤ì›Œë“œ ê²€ì¦)
                valid_article_count = 0
                
                for art in raw_articles:
                    # ë‚ ì§œ í•„í„°ë§
                    if not is_within_24h(art.get('pubDate')):
                        continue
                        
                    # ë³¸ë¬¸ í¬ë¡¤ë§
                    text, img = crawler.get_article_data(art['link'], target_keyword=kw)
                    
                    # í‚¤ì›Œë“œ ê²€ì¦ (ë³¸ë¬¸ì— í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê°€ì§œ ë‰´ìŠ¤ë¡œ ê°„ì£¼)
                    # ì˜ì–´ë¡œ ë²ˆì—­ëœ í‚¤ì›Œë“œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëŠìŠ¨í•˜ê²Œ ì²´í¬í•˜ê±°ë‚˜, 
                    # í¬ë¡¤ëŸ¬ ë ˆë²¨ì—ì„œ ì´ë¯¸ ì²´í¬í–ˆë‹¤ê³  ê°€ì •.
                    if text: 
                        full_contents.append(text)
                        valid_article_count += 1
                        
                        # ì´ë¯¸ì§€ í™•ë³´
                        if not main_image and img:
                            if img.startswith("http://"): img = img.replace("http://", "https://")
                            main_image = img
                            
                    # 5ê°œ ì •ë„ë§Œ ëª¨ìœ¼ë©´ ì¶©ë¶„
                    if valid_article_count >= 5:
                        break

                # 3. ì •ë³´ ë¶€ì¡± ì‹œ ì²˜ë¦¬
                if not full_contents:
                    print(f"      â˜ï¸ '{kw}': ìœ íš¨ ê¸°ì‚¬ ì—†ìŒ (Skip)")
                    continue

                # ==========================================
                # [4ë‹¨ê³„] AI ë¸Œë¦¬í•‘ ìƒì„± (ì˜ì–´, 5~20ì¤„)
                # ==========================================
                briefing = ai_engine.synthesize_briefing(kw, full_contents)
                
                # AIê°€ 'INVALID_DATA'ë¥¼ ì¤¬ë‹¤ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
                if not briefing:
                    print(f"      ğŸ—‘ï¸ '{kw}': ë‚´ìš© ë¶€ì‹¤ë¡œ íê¸°")
                    continue
                
                # í‰ì  ê³„ì‚° (ìˆœìœ„ ê¸°ë°˜, ìµœì†Œ 7.0)
                ai_score = round(9.9 - (rank * 0.1), 1)
                if ai_score < 7.0: ai_score = 7.0

                final_img = main_image or f"[https://placehold.co/600x400/111/cyan?text=](https://placehold.co/600x400/111/cyan?text=){kw}"

                news_item = {
                    "category": category,
                    "rank": rank + 1,
                    "keyword": kw,
                    "type": k_type,
                    "title": f"[{kw}] News Update",
                    "summary": briefing,
                    "link": None, # [ìš”ì²­] ë§í¬ None ì €ì¥
                    "image_url": final_img,
                    "score": ai_score,
                    "likes": 0, "dislikes": 0,
                    "created_at": datetime.now().isoformat(),
                    "published_at": datetime.now().isoformat()
                }
                category_news_list.append(news_item)
                time.sleep(0.5) 
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                continue

        # ==========================================
        # [5ë‹¨ê³„] ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì‚° ì €ì¥
        # ==========================================
        if category_news_list:
            print(f"   ğŸ’¾ ì €ì¥ ì‹œì‘: ì´ {len(category_news_list)}ê°œ ë‰´ìŠ¤ ìƒì„±ë¨")
            
            # 1. Live News: 1~30ìœ„ ìƒì„±ëœ ëª¨ë“  ë‰´ìŠ¤ ì €ì¥ (ì‚¬ëŒ í¬í•¨)
            repository.refresh_live_news(category, category_news_list)
            
            # 2. Trending Rankings: 'content' íƒ€ì…ì¸ ê²ƒë§Œ ê³¨ë¼ì„œ Top 10 ì €ì¥
            # (ì‚¬ëŒ ì´ë¦„ ì œì™¸, ê³¡ëª…/ì‘í’ˆëª…ë§Œ)
            content_only_list = [n for n in category_news_list if n.get('type') == 'content']
            if content_only_list:
                repository.update_sidebar_rankings(category, content_only_list[:10])
            else:
                # ë§Œì•½ content íƒ€ì…ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¹„ì›Œë‘ê¸°ë³´ë‹¤ ìƒìœ„ê¶Œ ëª‡ ê°œë¼ë„ ë„£ëŠ” ë¹„ìƒ ë¡œì§ (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
                # repository.update_sidebar_rankings(category, category_news_list[:5])
                pass
            
            # 3. Search Archive: í‰ì  7.0 ì´ìƒë§Œ ì €ì¥
            high_score_news = [n for n in category_news_list if n['score'] >= 7.0]
            if high_score_news:
                repository.save_to_archive(high_score_news)

    print("\nğŸ‰ ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

if __name__ == "__main__":
    run_master_scraper()
