import os
import sys
import json
import time
import random
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from datetime import datetime
from dotenv import load_dotenv
from groq import Groq
from urllib.parse import urljoin

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

supabase_url = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
supabase_key = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
groq_api_key = os.environ.get("GROQ_API_KEY")
naver_client_id = os.environ.get("NAVER_CLIENT_ID")
naver_client_secret = os.environ.get("NAVER_CLIENT_SECRET")

supabase: Client = create_client(supabase_url, supabase_key)
groq_client = Groq(api_key=groq_api_key)
AI_MODEL = "llama-3.3-70b-versatile"

# ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê¸° ìœ„í•œ ê³ ì • í—¤ë”
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Referer': 'https://news.naver.com/'
}

# 2. [ê¸°ëŠ¥ 1] ë„¤ì´ë²„ ì—°ì˜ˆ ë­í‚¹ 30ê°œ ìˆ˜ì§‘ (Selenium ìŠ¤íƒ€ì¼ ì§ì ‘ ìŠ¤í¬ë˜í•‘)
def get_naver_ranking_30():
    print("ğŸ“¡ ë„¤ì´ë²„ ì—°ì˜ˆ ì‹¤ì‹œê°„ ë­í‚¹ 30 ìˆ˜ì§‘ ì¤‘...")
    ranking_url = "https://entertain.naver.com/ranking"
    try:
        res = requests.get(ranking_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = []
        # ë„¤ì´ë²„ ë­í‚¹ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ íƒœê·¸ (ë„¤ì´ë²„ í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì‹œ ë³€ê²½ë  ìˆ˜ ìˆìŒ)
        # ë³´í†µ .rank_lst ë‚˜ .tit_area ì•ˆì˜ a íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        news_links = soup.select('.rank_lst li a.tit') or soup.select('.tit_area a')
        
        for i, a in enumerate(news_links[:30]):
            items.append({
                'title': a.get_text(strip=True),
                'link': urljoin(ranking_url, a['href']),
                'is_ranking': True,
                'rank': i + 1
            })
        return items
    except Exception as e:
        print(f"âš ï¸ ë­í‚¹ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# 3. [ê¸°ëŠ¥ 2] ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬ìš© ê²€ìƒ‰ì–´ ê¸°ë°˜ ìˆ˜ì§‘ (API ë°©ì‹)
def get_naver_api_news(keyword):
    import urllib.parse
    import urllib.request
    encText = urllib.parse.quote(keyword)
    url = f"https://openapi.naver.com/v1/search/news?query={encText}&display=10&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", naver_client_id)
    req.add_header("X-Naver-Client-Secret", naver_client_secret)
    try:
        res = urllib.request.urlopen(req)
        items = json.loads(res.read().decode('utf-8')).get('items', [])
        return [{'title': i['title'], 'link': i['link'], 'is_ranking': False} for i in items]
    except: return []

# 4. [ê¸°ëŠ¥ 3] ê¸°ì‚¬ ë³¸ë¬¸ ë° ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ì¶œ
def get_article_details(link):
    try:
        res = requests.get(link, headers=HEADERS, timeout=7)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ì´ë¯¸ì§€ ì°¾ê¸°
        og_image = soup.find('meta', property='og:image')
        img_url = og_image['content'] if og_image else None
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì°¾ê¸° (ìš”ì•½ìš©)
        content = soup.select_one('#dic_area, #articleBodyContents, .article_body')
        text = content.get_text(strip=True)[:1000] if content else ""
        
        return text, img_url
    except: return "", None

# 5. [ê¸°ëŠ¥ 4] AI í¸ì§‘ì¥: ìš”ì•½ ë° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (í™ˆ, ìŒì•…, ì˜í™”, ë“œë¼ë§ˆ, ì—°ì˜ˆ)
def ai_chief_editor(news_list):
    # AIì—ê²Œ ì „ë‹¬í•  í…ìŠ¤íŠ¸ êµ¬ì„±
    raw_text = "\n".join([f"[{i}] {n['title']}" for i, n in enumerate(news_list)])
    
    prompt = f"""
    Role: K-ENTER 24 Chief Editor.
    Task: Analyze the news and categorize into [Music, Movie, Drama, Celeb].
    Top 30 ranking news should also be assigned to Home.
    
    Raw News:
    {raw_text}
    
    JSON Output Format:
    {{
        "articles": [
            {{
                "original_index": 0,
                "category": "Music",
                "eng_title": "Headline in English",
                "summary": "1-2 sentence English summary",
                "reactions": {{"excitement": 70, "shock": 30, "sadness": 0}}
            }}
        ]
    }}
    """
    try:
        res = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=AI_MODEL,
            response_format={"type": "json_object"}
        )
        return json.loads(res.choices[0].message.content)
    except: return None

# 6. ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤
def run():
    # ëœë¤ íœ´ì‹ íš¨ê³¼ (1ë¶„ ~ 10ë¶„ ì‚¬ì´ ë¬´ì‘ìœ„ ëŒ€ê¸° í›„ ì‹œì‘)
    wait_time = random.randint(60, 600)
    print(f"ğŸ•’ ë³´ì•ˆì„ ìœ„í•´ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì‹œì‘í•©ë‹ˆë‹¤...")
    time.sleep(wait_time)

    print(f"=== {datetime.now()} í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ëª¨ë“œ ê°€ë™ ===")
    
    # 1ë‹¨ê³„: ë­í‚¹ ìˆ˜ì§‘
    all_raw_news = get_naver_ranking_30()
    
    # 2ë‹¨ê³„: ëª¨ìë€ ì¹´í…Œê³ ë¦¬ ë³´ì¶© (ìŒì•…, ì˜í™”, ë“œë¼ë§ˆ ë“±)
    keywords = ["K-POP ì‹ ê³¡", "í•œêµ­ ì˜í™” ê°œë´‰", "í•œêµ­ ë“œë¼ë§ˆ í™”ì œ"]
    for kw in keywords:
        all_raw_news.extend(get_naver_api_news(kw))
    
    # 3ë‹¨ê³„: AI ë¶„ì„
    analysis = ai_chief_editor(all_raw_news)
    if not analysis: return

    # 4ë‹¨ê³„: ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ë° DB ì €ì¥
    saved = 0
    for art in analysis.get('articles', []):
        idx = art['original_index']
        if idx >= len(all_raw_news): continue
        
        item = all_raw_news[idx]
        
        # ì¤‘ë³µ ì²´í¬
        if supabase.table("live_news").select("id").eq("link", item['link']).execute().data:
            continue
            
        body, img = get_article_details(item['link'])
        if not img: img = f"https://placehold.co/600x400/111/cyan?text={art['category']}"

        try:
            data = {
                "category": art['category'], # ìŒì•…, ì˜í™”, ë“œë¼ë§ˆ, ì—°ì˜ˆ ë“±
                "title": art['eng_title'],
                "summary": art['summary'],
                "link": item['link'],
                "image_url": img,
                "reactions": art['reactions'],
                "is_ranking": item.get('is_ranking', False),
                "created_at": datetime.now().isoformat()
            }
            supabase.table("live_news").insert(data).execute()
            saved += 1
            print(f"âœ… ì €ì¥: {art['eng_title'][:30]}...")
        except: pass

    print(f"=== ì‘ì—… ì™„ë£Œ: {saved}ê°œ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ===")

if __name__ == "__main__":
    run()
