import os
import sys
import json
import time
import requests
import re
from supabase import create_client, Client
from datetime import datetime, timedelta
from dateutil.parser import isoparse 
from dotenv import load_dotenv
from groq import Groq
from bs4 import BeautifulSoup

load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

supabase: Client = create_client(os.environ.get("NEXT_PUBLIC_SUPABASE_URL"), os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY"))
groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

CATEGORY_MAP = {
    "k-pop": ["ì»´ë°±", "ë¹Œë³´ë“œ", "ì•„ì´ëŒ", "ë®¤ì§", "ë¹„ë””ì˜¤", "ì±Œë¦°ì§€", "í¬í† ì¹´ë“œ", "ì›”ë“œíˆ¬ì–´", "ê°€ìˆ˜"],
    "k-drama": ["ë“œë¼ë§ˆ", "ì‹œì²­ë¥ ", "ë„·í”Œë¦­ìŠ¤", "OTT", "ë°°ìš°", "ìºìŠ¤íŒ…", "ëŒ€ë³¸ë¦¬ë”©", "ì¢…ì˜"],
    "k-movie": ["ì˜í™”", "ê°œë´‰", "ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "ì‹œì‚¬íšŒ", "ì˜í™”ì œ", "ê´€ê°", "ë¬´ëŒ€ì¸ì‚¬"],
    "k-entertain": ["ì˜ˆëŠ¥", "ìœ íŠœë¸Œ", "ê°œê·¸ë§¨", "ì½”ë¯¸ë””ì–¸", "ë°©ì†¡", "ê°œê·¸ìš°ë¨¼"],
    "k-culture": ["í‘¸ë“œ", "ë·°í‹°", "ì›¹íˆ°", "íŒì—…ìŠ¤í† ì–´", "íŒ¨ì…˜", "ìŒì‹", "í•´ì™¸ë°˜ì‘"]
}

# [ê¸°ì¡´ ìœ ì§€] ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš©í•˜ì§€ ì•Šë”ë¼ë„ ê¸°ì¡´ ì½”ë“œ ë³´ì¡´ì„ ìœ„í•´ ë‚¨ê²¨ë‘ )
STOPWORDS = {
    "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by", 
    "from", "up", "about", "into", "over", "after", "is", "are", "was", "were", "be", "been", 
    "has", "have", "had", "it", "its", "they", "their", "this", "that", "these", "those", 
    "new", "news", "official", "update", "korea", "korean", "top", "best", "hot", "reveals",
    "releases", "drops", "teaser", "mv", "video", "photo", "poster", "trailer", "scene",
    "netizens", "fans", "reaction", "review", "rank", "list", "vs", "kpop", "kdrama", "drama", "movie"
}

# [í•µì‹¬ 1] ë¯¸ëž˜ì§€í–¥ì  AI ëª¨ë¸ ìžë™ ì„ íƒ í•¨ìˆ˜ (Smart Sort)
def get_best_model():
    """
    Groq APIì—ì„œ í˜„ìž¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì¡°íšŒí•˜ê³ ,
    'ìµœì‹  ë²„ì „(ìˆ«ìž)' + 'í° íŒŒë¼ë¯¸í„°(70b)' + 'ì„ í˜¸ íŒ¨ë°€ë¦¬(Llama)' ìˆœìœ¼ë¡œ ìžë™ ì •ë ¬í•˜ì—¬ ë°˜í™˜.
    ë‚˜ì¤‘ì— Llama 4.0ì´ ë‚˜ì™€ë„ ì½”ë“œ ìˆ˜ì • ì—†ì´ ìžë™ìœ¼ë¡œ 1ìˆœìœ„ê°€ ë¨.
    """
    try:
        # 1. Groqì—ì„œ í˜„ìž¬ ì‚´ì•„ìžˆëŠ” ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        models_raw = groq_client.models.list()
        available_models = [m.id for m in models_raw.data]
        
        # 2. ëª¨ë¸ í•„í„°ë§ ë° ì ìˆ˜ ë§¤ê¸°ê¸° ë¡œì§
        def model_scorer(model_id):
            score = 0
            model_id = model_id.lower()
            
            # (1) ì„ í˜¸í•˜ëŠ” ëª¨ë¸ ê°€ë¬¸ (Family)
            if "llama" in model_id: score += 1000
            elif "mixtral" in model_id: score += 500
            elif "gemma" in model_id: score += 100
            
            # (2) ë²„ì „ ìˆ«ìž ì¶”ì¶œ (ì˜ˆ: llama-3.3 -> 3.3)
            # ì •ê·œì‹ìœ¼ë¡œ ìˆ«ìž.ìˆ«ìž íŒ¨í„´ì„ ì°¾ì•„ì„œ ì ìˆ˜ì— ë°˜ì˜ (3.3 > 3.1)
            version_match = re.search(r'(\d+\.?\d*)', model_id)
            if version_match:
                try:
                    version = float(version_match.group(1))
                    score += version * 100 
                except: pass

            # (3) íŒŒë¼ë¯¸í„° í¬ê¸° (í´ìˆ˜ë¡ ë˜‘ë˜‘í•¨)
            if "70b" in model_id: score += 50
            elif "8b" in model_id: score += 10
            
            # (4) Versatile ëª¨ë¸ ì„ í˜¸ (ë²”ìš©ì„±)
            if "versatile" in model_id: score += 5

            return score

        # 3. ì ìˆ˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        available_models.sort(key=model_scorer, reverse=True)
        
        # ìƒìœ„ 3ê°œ ëª¨ë¸ë§Œ ë¡œê·¸ì— í‘œì‹œ
        print(f"ðŸ¤– AI ëª¨ë¸ ìžë™ ì„ íƒ ì™„ë£Œ: {available_models[:3]}")
        return available_models

    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨ (ì•ˆì „ëª¨ë“œ ì§„ìž…): {e}")
        # APIê°€ ì£½ì—ˆì„ ë•Œë¥¼ ëŒ€ë¹„í•œ ìµœí›„ì˜ ì•ˆì „ìž¥ì¹˜ (í•˜ë“œì½”ë”©)
        return ["llama-3.3-70b-versatile", "mixtral-8x7b-32768"]

# ì „ì—­ ë³€ìˆ˜ë¡œ ìµœì ì˜ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
MODELS_TO_TRY = get_best_model()

def get_naver_api_news(keyword):
    import urllib.parse, urllib.request
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    try:
        res = urllib.request.urlopen(req)
        return json.loads(res.read().decode('utf-8')).get('items', [])
    except: return []

# [í•µì‹¬ 2] ì—‰ëš±í•œ ì‚¬ì§„ ë°©ì§€ ë¡œì§ (ë³¸ë¬¸ ìš°ì„  + í•„í„°ë§)
def get_article_image(link):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        res = requests.get(link, headers=headers, timeout=3)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        candidates = []

        # 1. ë³¸ë¬¸ ì˜ì—­(div) ì•ˆì˜ ì´ë¯¸ì§€ ìµœìš°ì„  ì¶”ì¶œ
        # ë„¤ì´ë²„ ì—°ì˜ˆ ë‰´ìŠ¤ëŠ” ë³´í†µ id="img1" ë˜ëŠ” id="dic_area" ë‚´ë¶€ì— ë³¸ë¬¸ì´ ìžˆìŒ
        main_content = soup.select_one('#dic_area, #articleBodyContents, .article_view, #articeBody, .news_view')
        
        if main_content:
            # ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ íƒœê·¸ ìˆ˜ì§‘
            imgs = main_content.find_all('img')
            for i in imgs:
                src = i.get('src') or i.get('data-src')
                if src and 'http' in src:
                    # width ì†ì„±ì´ ìžˆëŠ”ë° 200px ë¯¸ë§Œì´ë©´ ì•„ì´ì½˜ì¼ í™•ë¥  ë†’ìŒ -> ì œì™¸
                    width = i.get('width')
                    if width and width.isdigit() and int(width) < 200:
                        continue
                    candidates.append(src)

        # 2. ë©”íƒ€ íƒœê·¸ (og:image) - ë³¸ë¬¸ ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ì„ ë•Œ ì‚¬ìš©
        og = soup.find('meta', property='og:image')
        if og and og.get('content'):
            candidates.append(og['content'])

        # 3. í•„í„°ë§ ë° ìµœì¢… ì„ íƒ
        for img_url in candidates:
            # URLì— íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ íŒ¨ìŠ¤ (ë¡œê³ , ì•„ì´ì½˜, ë°°ë„ˆ ë“±)
            # Blackpink ê¸°ì‚¬ì— 'hot_click_bts.jpg' ê°™ì€ ì‚¬ì´ë“œë°” ë°°ë„ˆê°€ ê±¸ë¦¬ëŠ” ê²ƒì„ ë°©ì§€
            bad_keywords = r'logo|icon|button|share|banner|thumb|profile|default|ranking|news_stand|ssl.pstatic.net'
            if re.search(bad_keywords, img_url, re.IGNORECASE):
                continue
            
            # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ 'ê¹¨ë—í•œ' ì´ë¯¸ì§€ë¥¼ ë°˜í™˜
            return img_url

        # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜ (ë‚˜ì¤‘ì— placeholder ì²˜ë¦¬)
        return None
    except: return None

def ai_category_editor(category, news_batch):
    if not news_batch: return []
    
    # AI ìž…ë ¥ ë°ì´í„° 50ê°œë¡œ ì œí•œ (ì†ë„/ë¹„ìš© ìµœì í™”)
    limited_batch = news_batch[:50]
    raw_text = "\n".join([f"[{i}] {n['title']}" for i, n in enumerate(limited_batch)])
    
    prompt = f"""
    Task: Select exactly 30 news items for '{category}'. If fewer than 30, select ALL valid ones.
    
    Constraints: 
    1. Rank 1-30.
    2. English title & 3-line English summary.
    3. AI Score (0.0-10.0).
    4. Return JSON format strictly.

    News List:
    {raw_text}

    Output JSON Format:
    {{
        "articles": [
            {{ "original_index": 0, "rank": 1, "category": "{category}", "eng_title": "...", "summary": "...", "score": 9.5 }}
        ]
    }}
    """
    
    # ë™ì ìœ¼ë¡œ ë¡œë“œí•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„
    for model in MODELS_TO_TRY:
        try:
            res = groq_client.chat.completions.create(
                messages=[{"role": "system", "content": f"You are a K-Enter Editor for {category}."},
                          {"role": "user", "content": prompt}], 
                model=model, 
                response_format={"type": "json_object"}
            )
            data = json.loads(res.choices[0].message.content)
            articles = data.get('articles', [])
            if articles: return articles
        except Exception as e:
            # 429(Too Many Requests)ë‚˜ 400(Bad Request) ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ ì°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°
            print(f"      âš ï¸ {model} ì‹¤íŒ¨ ({str(e)[:60]}...). ë‹¤ìŒ ëª¨ë¸ ì‹œë„.")
            continue
    return []

# [í•µì‹¬ 3] AI ê¸°ë°˜ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ í•¨ìˆ˜ (ë‹¨ì–´ ì„¸ê¸° X -> AI ë¶„ì„ O)
def update_hot_keywords():
    print("ðŸ“Š AI í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹œìž‘...")
    
    # 1. DBì—ì„œ ìµœê·¼ ê¸°ì‚¬ ì œëª© ê°€ì ¸ì˜¤ê¸° (ìµœì‹ ìˆœ 100ê°œ)
    # ë„ˆë¬´ ì˜›ë‚  ê¸°ì‚¬ê¹Œì§€ ê°€ì ¸ì˜¤ë©´ íŠ¸ë Œë“œê°€ í¬ì„ë˜ë¯€ë¡œ ìµœì‹  100ê°œë¡œ ì œí•œ
    res = supabase.table("live_news").select("title").order("created_at", desc=True).limit(100).execute()
    titles = [item['title'] for item in res.data]
    
    if not titles:
        print("   âš ï¸ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. AIì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ ìž‘ì„±
    titles_text = "\n".join([f"- {t}" for t in titles])
    
    prompt = f"""
    Analyze the following K-Entertainment news titles and identify the TOP 10 most trending keywords.
    
    [Rules]
    1. Extract specific Entities: Person Name (e.g., "Lee Min-ho", NOT "Lee"), Group Name (e.g., "BTS"), Drama/Movie Title (e.g., "Squid Game").
    2. Merge related concepts: If "Jin" and "BTS" are both popular, use "BTS Jin".
    3. EXCLUDE generic words: Do NOT use words like "Variety", "Actor", "K-pop", "Review", "Netizens", "Update", "Official", "Comeback", "Teaser".
    4. Return JSON format with 'keyword' and estimated 'count' (importance score 1-100).

    [Titles]
    {titles_text}

    [Output Format JSON]
    {{
        "keywords": [
            {{ "keyword": "BTS Jin", "count": 95, "rank": 1 }},
            {{ "keyword": "Squid Game 2", "count": 80, "rank": 2 }}
        ]
    }}
    """

    # 3. AI ëª¨ë¸ í˜¸ì¶œ (ê°€ìž¥ ë˜‘ë˜‘í•œ ëª¨ë¸ ì‚¬ìš©)
    for model in MODELS_TO_TRY:
        try:
            res = groq_client.chat.completions.create(
                messages=[{"role": "system", "content": "You are a K-Trend Analyst."},
                          {"role": "user", "content": prompt}], 
                model=model, 
                response_format={"type": "json_object"}
            )
            
            # ê²°ê³¼ íŒŒì‹±
            result = json.loads(res.choices[0].message.content)
            keywords = result.get('keywords', [])
            
            if not keywords:
                continue

            print(f"   ðŸ”¥ AIê°€ ì¶”ì¶œí•œ ì§„ì§œ íŠ¸ë Œë“œ: {[k['keyword'] for k in keywords[:5]]}...")

            # 4. DB ì—…ë°ì´íŠ¸
            supabase.table("trending_keywords").delete().neq("id", 0).execute()
            
            insert_data = []
            for item in keywords:
                insert_data.append({
                    "keyword": item['keyword'],
                    "count": item['count'],
                    "rank": item['rank'],
                    "updated_at": datetime.now().isoformat()
                })
            
            if insert_data:
                supabase.table("trending_keywords").insert(insert_data).execute()
                print("   âœ… í‚¤ì›Œë“œ ëž­í‚¹ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
                return # ì„±ê³µí•˜ë©´ ì¢…ë£Œ

        except Exception as e:
            print(f"      âš ï¸ {model} ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue

def run():
    print("ðŸš€ 7ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™ (ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ë§ + ì •ë°€ ì´ë¯¸ì§€ + í‚¤ì›Œë“œ ë¶„ì„)...")
    
    for category, keywords in CATEGORY_MAP.items():
        print(f"ðŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘...")

        # 1. ìˆ˜ì§‘
        raw_news = []
        for kw in keywords: raw_news.extend(get_naver_api_news(kw))
        
        # 2. ì¤‘ë³µ ì œê±° (DB ë¹„êµ)
        db_res = supabase.table("live_news").select("link").eq("category", category).execute()
        db_links = {item['link'] for item in db_res.data}
        new_candidate_news = [n for n in raw_news if n['link'] not in db_links]
        new_candidate_news = list({n['link']: n for n in new_candidate_news}.values())
        
        print(f"   ðŸ”Ž ìˆ˜ì§‘: {len(raw_news)}ê°œ -> ì‹ ê·œ í›„ë³´: {len(new_candidate_news)}ê°œ")

        # 3. AI ì„ ë³„
        selected = ai_category_editor(category, new_candidate_news)
        num_new = len(selected)
        print(f"   ã„´ AI ì„ ë³„ ì™„ë£Œ: {num_new}ê°œ")

        if num_new > 0:
            new_data_list = []
            for art in selected:
                idx = art['original_index']
                if idx >= len(new_candidate_news): continue
                
                orig = new_candidate_news[idx]
                
                # [ì´ë¯¸ì§€ ì¶”ì¶œ] ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©
                img = get_article_image(orig['link']) 
                if not img: 
                    img = f"https://placehold.co/600x400/111/cyan?text={category}"

                new_data_list.append({
                    "rank": art['rank'], "category": category, "title": art['eng_title'],
                    "summary": art['summary'], "link": orig['link'], "image_url": img,
                    "score": art['score'], "likes": 0, "dislikes": 0, "created_at": datetime.now().isoformat()
                })
            
            # 7. ì €ìž¥ (Upsert)
            if new_data_list:
                supabase.table("live_news").upsert(new_data_list, on_conflict="link").execute()
                print(f"   âœ… ì‹ ê·œ {len(new_data_list)}ê°œ ì‚½ìž… ì™„ë£Œ.")

        # 4~6. ìŠ¬ë¡¯ ì²´í¬ ë° ì‚­ì œ (30ê°œ ìœ ì§€ ë¡œì§)
        res = supabase.table("live_news").select("id", "created_at", "score").eq("category", category).execute()
        current_articles = res.data
        
        if len(current_articles) > 30:
            now = datetime.now()
            threshold = now - timedelta(hours=24)
            
            old_articles = []
            fresh_articles = []
            for a in current_articles:
                try:
                    dt_obj = isoparse(a['created_at']).replace(tzinfo=None)
                except:
                    dt_obj = datetime(2000, 1, 1)

                if dt_obj < threshold: old_articles.append(a)
                else: fresh_articles.append(a)
            
            delete_ids = []
            current_count = len(current_articles)
            
            # ì˜¤ëž˜ëœ ê²ƒ ì‚­ì œ
            old_articles.sort(key=lambda x: x['created_at'])
            for oa in old_articles:
                if current_count <= 30: break
                delete_ids.append(oa['id'])
                current_count -= 1
            
            # ì ìˆ˜ ë‚®ì€ ê²ƒ ì‚­ì œ
            if current_count > 30:
                fresh_articles.sort(key=lambda x: x['score'])
                for fa in fresh_articles:
                    if current_count <= 30: break
                    delete_ids.append(fa['id'])
                    current_count -= 1

            if delete_ids:
                supabase.table("live_news").delete().in_("id", delete_ids).execute()
                print(f"   ðŸ§¹ ìŠ¬ë¡¯ ì¡°ì •: {len(delete_ids)}ê°œ ì‚­ì œ ì™„ë£Œ.")

    # [ì¶”ê°€] ëª¨ë“  ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ê°€ ëë‚œ í›„ í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
    update_hot_keywords()
    
    print(f"ðŸŽ‰ ìž‘ì—… ì™„ë£Œ.")

if __name__ == "__main__":
    run()
