import sys
import os
import time
from datetime import datetime, timedelta
from dateutil import parser
from dotenv import load_dotenv

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì°¸ì¡° ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS

load_dotenv()

# ìœ ë£Œ ë²„ì „ì˜ í™”ë ¥ì„ í™œìš©í•´ ë¶„ì„ ë²”ìœ„ë¥¼ 30ìœ„ê¹Œì§€ í™•ëŒ€
TARGET_RANK_LIMIT = 30 

def is_within_24h(date_str):
    if not date_str: return False
    try:
        pub_date = parser.parse(date_str)
        if pub_date.tzinfo:
            pub_date = pub_date.replace(tzinfo=None)
        now = datetime.now()
        diff = now - pub_date
        return diff <= timedelta(hours=24)
    except:
        return False

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # [1ë‹¨ê³„] ì”¨ì•— ë°ì´í„° ìˆ˜ì§‘ (24ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ ìš”ì•½ë³¸ë“¤)
        raw_text_data = [] 
        
        try:
            for seed in seeds:
                # ì‹œë“œë³„ë¡œ ê¸°ì‚¬ë¥¼ ë„‰ë„‰íˆ ê°€ì ¸ì˜´ (ìœ ë£Œ ë²„ì „ ëŒ€ì‘)
                news_items = crawler.get_naver_api_news(seed, display=20)
                for item in news_items:
                    if is_within_24h(item.get('pubDate')):
                        combined_text = f"Title: {item['title']}\nSummary: {item['description']}"
                        raw_text_data.append(combined_text)
            
            # AI ì…ë ¥ìš© ë°ì´í„° ì œí•œ (Groq ìœ ë£Œ ëª¨ë¸ì€ 60ê°œ ê¸°ì‚¬ ë¶„ëŸ‰ë„ ì¶©ë¶„íˆ ì†Œí™”)
            raw_text_data = raw_text_data[:60]
            print(f"   ğŸŒ± 24ì‹œê°„ ë‚´ ìœ íš¨ ê¸°ì‚¬ ìˆ˜ì§‘: {len(raw_text_data)}ê°œ")
            
            # ê¸°ì‚¬ê°€ 1ê°œë¼ë„ ìˆìœ¼ë©´ íŠ¸ë Œë“œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.
            if len(raw_text_data) < 1:
                print("   âš ï¸ ê¸°ì‚¬ê°€ ì „í˜€ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue
                
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            continue
        
        # [2ë‹¨ê³„] AI í‚¤ì›Œë“œ ì¶”ì¶œ (Strict Mode ì ìš©ëœ ai_engine í˜¸ì¶œ)
        top_entities = ai_engine.extract_top_entities(category, "\n".join(raw_text_data))
        
        if not top_entities: 
            print("   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ í˜¹ì€ ìœ íš¨í•œ í‚¤ì›Œë“œ ì—†ìŒ")
            continue
            
        print(f"   ğŸ’ ìœ íš¨ í‚¤ì›Œë“œ (Top 5): {', '.join([e['keyword'] for e in top_entities[:5]])}...")

        # [3ë‹¨ê³„] í‚¤ì›Œë“œë³„ ì‹¬ì¸µ ë¶„ì„ (30ìœ„ê¹Œì§€ ë¶„ì„ ìˆ˜í–‰)
        category_news_list = []
        target_list = top_entities[:TARGET_RANK_LIMIT]
        
        for rank, entity in enumerate(target_list):
            kw = entity.get('keyword')
            k_type = entity.get('type', 'content') # person ë˜ëŠ” content
            
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ({k_type}) ë¶„ì„ ì¤‘...")
            
            try:
                # íŠ¹ì • í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ (ìœ ë£Œ ë²„ì „ì´ë¯€ë¡œ 50ê°œì”© í™•ì¸)
                raw_articles = crawler.get_naver_api_news(kw, display=50)
                if not raw_articles: continue

                full_contents = []
                main_image = None
                valid_article_count = 0
                
                for art in raw_articles:
                    if not is_within_24h(art.get('pubDate')): continue
                    
                    # [ì¤‘ìš”] target_keywordë¥¼ ë¹¼ì„œ 'BTS'ì™€ 'ë°©íƒ„ì†Œë…„ë‹¨' í˜¼ìš©ì„ í—ˆìš©í•©ë‹ˆë‹¤.
                    # ë“¤ì—¬ì“°ê¸° 4ì¹¸(tab ì•„ë‹˜)ì„ ì—„ê²©íˆ ì¤€ìˆ˜í–ˆìŠµë‹ˆë‹¤.
                    text, img = crawler.get_article_data(art['link'])
                    
                    if text: 
                        full_contents.append(text)
                        valid_article_count += 1
                        # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ì„¤ì •
                        if not main_image and img:
                            if img.startswith("http://"): 
                                img = img.replace("http://", "https://")
                            main_image = img
                            
                    # í•œ í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 30ê°œ ê¸°ì‚¬ê¹Œì§€ ë¶„ì„ (ìœ ë£Œ ë²„ì „ í™”ë ¥ í™œìš©)
                    if valid_article_count >= 30: 
                        break

                # ê¸°ì‚¬ê°€ ë‹¨ 1ê°œë¼ë„ ìˆ˜ì§‘ë˜ì—ˆë‹¤ë©´ AI ë¸Œë¦¬í•‘ ìƒì„±ì„ ì§„í–‰í•©ë‹ˆë‹¤.
                if not full_contents:
                    print(f"      â˜ï¸ '{kw}': ìœ íš¨ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨ (Skip)")
                    continue

                # [4ë‹¨ê³„] AI ë¸Œë¦¬í•‘ ìƒì„±
                briefing = ai_engine.synthesize_briefing(kw, full_contents)
                
                if not briefing:
                    print(f"      ğŸ—‘ï¸ '{kw}': ë¸Œë¦¬í•‘ ìƒì„± ì‹¤íŒ¨ë¡œ íê¸°")
                    continue
                
                # ìˆœìœ„ì— ë”°ë¥¸ ì ìˆ˜ ë¶€ì—¬ (ê¸°ë³¸ 7.0ì  ì´ìƒ ìœ ì§€)
                ai_score = round(9.9 - (rank * 0.1), 1)
                if ai_score < 7.0: ai_score = 7.0

                # ì´ë¯¸ì§€ê°€ ì—†ì„ ê²½ìš° í”Œë ˆì´ìŠ¤í™€ë” ì´ë¯¸ì§€ ì‚¬ìš©
                final_img = main_image or f"https://placehold.co/600x400/111/cyan?text={kw}"

                news_item = {
                    "category": category,
                    "rank": rank + 1,
                    "keyword": kw,
                    "type": k_type,
                    "title": f"[{kw}] News Update",
                    "summary": briefing,
                    "link": None, # ë¸Œë¦¬í•‘ëœ ë‰´ìŠ¤ì´ë¯€ë¡œ ì›ë¬¸ ë§í¬ëŠ” ìƒëµí•˜ê±°ë‚˜ ì²« ê¸°ì‚¬ ë§í¬ ì‚¬ìš© ê°€ëŠ¥
                    "image_url": final_img,
                    "score": ai_score,
                    "likes": 0, "dislikes": 0,
                    "created_at": datetime.now().isoformat(),
                    "published_at": datetime.now().isoformat()
                }
                category_news_list.append(news_item)
                
                # Groq ìœ ë£Œ ë²„ì „ì€ RPMì´ ë†’ìœ¼ë¯€ë¡œ ëŒ€ê¸° ì‹œê°„ì„ 2ì´ˆì—ì„œ 0.5ì´ˆë¡œ ë‹¨ì¶• ê°€ëŠ¥
                time.sleep(0.5) 
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                continue

        # [5ë‹¨ê³„] ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì‚° ì €ì¥
        if category_news_list:
            print(f"   ğŸ’¾ ì €ì¥ ì‹œì‘: ì´ {len(category_news_list)}ê°œ")
            
            # 1. Live News: ëª¨ë“  ë‰´ìŠ¤ ì €ì¥
            repository.refresh_live_news(category, category_news_list)
            
            # 2. Trending Rankings: ì‚¬ì´ë“œë°” ë­í‚¹ ì—…ë°ì´íŠ¸
            content_only_list = [n for n in category_news_list if n.get('type') == 'content']
            
            final_ranking_list = []
            source_list = content_only_list if len(content_only_list) >= 3 else category_news_list

            for new_rank, item in enumerate(source_list[:10]):
                ranked_item = item.copy()
                ranked_item['rank'] = new_rank + 1
                final_ranking_list.append(ranked_item)
                
            repository.update_sidebar_rankings(category, final_ranking_list)
            
            # 3. Search Archive: ì•„ì¹´ì´ë¸Œ ì €ì¥
            repository.save_to_archive(category_news_list)

    print("\nğŸ‰ ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

if __name__ == "__main__":
    run_master_scraper()
