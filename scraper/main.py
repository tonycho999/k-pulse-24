import os
import sys
import json
import time
import requests
from supabase import create_client, Client
from datetime import datetime, timedelta
from dotenv import load_dotenv
from groq import Groq

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •
load_dotenv()
sys.stdout.reconfigure(encoding='utf-8')

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase: Client = create_client(os.environ.get("NEXT_PUBLIC_SUPABASE_URL"), os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY"))
groq_client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

MODELS_TO_TRY = ["llama-3.3-70b-versatile", "llama-3.1-70b-versatile", "llama-3.1-8b-instant"]

# ê²€ìƒ‰ í‚¤ì›Œë“œ (K-Culture ì „ë°˜ì„ í¬ê´„)
SEARCH_KEYWORDS = [
    "ì»´ë°±", "ë¹Œë³´ë“œ", "ë°ë·”", "ì›”ë“œíˆ¬ì–´", "ë…ì ", "ê°€ìˆ˜", "ì•„ì´ëŒ",
    "ë®¤ì§ë¹„ë””ì˜¤", "ì±Œë¦°ì§€", "ìœ í–‰", "ì— ì¹´", "í¬í† ì¹´ë“œ",
    "ì‹œì²­ë¥ ", "ì¢…ì˜", "ë„·í”Œë¦­ìŠ¤", "ëŒ€ë³¸ë¦¬ë”©", "ë°°ìš°",
    "ë“œë¼ë§ˆ", "ìºìŠ¤íŒ…", "OTT", "ì œì‘ë°œí‘œíšŒ", "ë°˜ì „ ê²°ë§", "ê°œë´‰",
    "ì˜í™”", "ê´€ê°ìˆ˜", "ë°•ìŠ¤ì˜¤í”¼ìŠ¤", "ì‹œì‚¬íšŒ", "ë¬´ëŒ€ì¸ì‚¬",
    "ì˜ˆëŠ¥", "ëŒ€ìƒ í›„ë³´", "ìœ íŠœë¸Œ", "ê°œê·¸ë§¨", "ê°œê·¸ìš°ë¨¼", "ì½”ë¯¸ë””ì–¸",
    "í‘¸ë“œ", "í•´ì™¸ ë°˜ì‘", "ë·°í‹°", "íŒì—…ìŠ¤í† ì–´", "ì›¹íˆ°", "íŒ¨ì…˜", "ìŒì‹"
]

def get_naver_api_news(keyword):
    """ë„¤ì´ë²„ APIë¥¼ í†µí•´ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìµœëŒ€ 100ê±´)"""
    import urllib.parse, urllib.request
    url = f"https://openapi.naver.com/v1/search/news?query={urllib.parse.quote(keyword)}&display=100&sort=sim"
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", os.environ.get("NAVER_CLIENT_ID"))
    req.add_header("X-Naver-Client-Secret", os.environ.get("NAVER_CLIENT_SECRET"))
    try:
        res = urllib.request.urlopen(req)
        return json.loads(res.read().decode('utf-8')).get('items', [])
    except: return []

def get_article_image(link):
    """ê¸°ì‚¬ ì›ë¬¸ì—ì„œ og:image ì¶”ì¶œ"""
    from bs4 import BeautifulSoup
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        res = requests.get(link, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        og_image = soup.find('meta', property='og:image')
        return og_image['content'] if og_image else None
    except: return None

def ai_chief_editor(news_batch):
    """AIë¥¼ í†µí•´ ë‰´ìŠ¤ ì„ ë³„ ë° ë­í‚¹ ë¶€ì—¬ (ìµœëŒ€ 200ê°œ)"""
    # ì „ì†¡ ë°ì´í„° í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ ì œëª© ìœ„ì£¼ë¡œ ì „ë‹¬
    raw_text = "\n".join([f"[{i}] {n['title']}" for i, n in enumerate(news_batch)])
    
    prompt = f"""
    Task: Analyze these {len(news_batch)} news items. 
    1. Select as many news items as possible (UP TO 200) and rank them by buzzworthiness.
    2. Ensure a balanced distribution across categories: [k-pop, k-drama, k-movie, k-entertain, k-culture].
    3. Categorize each item accurately based on the content.
    4. Generate a ONE-SENTENCE "Global Insight" based on the REAL trends found in these news titles.

    Output JSON Format:
    {{
        "global_insight": "Actual trend summary...",
        "articles": [
            {{ "original_index": 0, "rank": 1, "category": "k-pop", "eng_title": "Translated Title", "summary": "3-line English summary", "score": 9.5 }}
        ]
    }}
    """
    
    for model in MODELS_TO_TRY:
        try:
            print(f"ğŸ¤– AI ë¶„ì„ ì¤‘... (ëª¨ë¸: {model})")
            res = groq_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}], 
                model=model, 
                response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content)
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ {model} ì‹¤íŒ¨: {e}")
            continue
    return None

def run():
    print("ğŸš€ ë‰´ìŠ¤ ì—”ì§„ ê°€ë™...")

    # 1. 24ì‹œê°„ ì§€ë‚œ ë‰´ìŠ¤ ì‚­ì œ
    time_threshold = (datetime.now() - timedelta(hours=24)).isoformat()
    print(f"ğŸ§¹ 24ì‹œê°„ ê²½ê³¼ ë°ì´í„° ì‚­ì œ ì¤‘... (ê¸°ì¤€: {time_threshold})")
    supabase.table("live_news").delete().lt("created_at", time_threshold).execute()

    # 2. ì‚­ì œ ì „, í˜„ì¬ 'ì¢‹ì•„ìš”' Top 10 ì•„ì¹´ì´ë¸Œ ì €ì¥
    print("â­ í˜„ì¬ ì¢‹ì•„ìš” Top 10 ì•„ì¹´ì´ë¸Œ ë°±ì—… ì¤‘...")
    try:
        top_voted = supabase.table("live_news").select("*").order("likes", desc=True).limit(10).execute()
        for item in top_voted.data:
            archive_data = {
                "original_link": item['link'],
                "category": item['category'],
                "title": item['title'],
                "summary": item['summary'],
                "image_url": item['image_url'],
                "score": item['score'],
                "archive_reason": "Top 10 Likes"
            }
            supabase.table("search_archive").upsert(archive_data, on_conflict="original_link").execute()
    except Exception as e:
        print(f"âš ï¸ ì¢‹ì•„ìš” ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨: {e}")

    # 3. ì‹ ê·œ ë‰´ìŠ¤ ìˆ˜ì§‘
    all_news_raw = []
    for kw in SEARCH_KEYWORDS:
        all_news_raw.extend(get_naver_api_news(kw))
    
    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    unique_news = {n['link']: n for n in all_news_raw}.values()
    all_news = list(unique_news)
    print(f"ğŸ” ì¤‘ë³µ ì œê±° í›„ {len(all_news)}ê±´ í™•ë³´. AI ì„ ë³„ ì‹œì‘...")

    # 4. AI í¸ì§‘ì¥ í˜¸ì¶œ (ìµœëŒ€ 200ê°œ ì„ ë³„)
    result = ai_chief_editor(all_news)
    if not result:
        print("âŒ AI ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    global_insight = result.get('global_insight', "Global K-Wave is reaching new heights across all sectors.")
    articles = result.get('articles', [])
    
    # 5. ì‹¤ì‹œê°„ ë­í‚¹ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ê¸°ì¡´ live_news ì‚­ì œ (ID 0 ì œì™¸)
    supabase.table("live_news").delete().neq("id", "00000000-0000-0000-0000-000000000000").execute()

    # 6. ê²°ê³¼ ì €ì¥
    saved = 0
    for art in articles:
        idx = art['original_index']
        if idx >= len(all_news): continue
        orig = all_news[idx]
        
        img = get_article_image(orig['link'])
        if not img: img = f"https://placehold.co/600x400/111/cyan?text={art['category']}"

        data = {
            "rank": art['rank'],
            "category": art['category'],
            "title": art['eng_title'],
            "summary": art['summary'],
            "link": orig['link'],
            "image_url": img,
            "score": art['score'],
            "insight": global_insight,
            "likes": 0, "dislikes": 0,
            "created_at": datetime.now().isoformat()
        }
        
        # live_news í…Œì´ë¸”ì— ì €ì¥
        supabase.table("live_news").insert(data).execute()
        
        # ë­í‚¹ 10ìœ„ê¶Œ ì´ë‚´ ì•„ì¹´ì´ë¸Œ ì˜êµ¬ ì €ì¥
        if art['rank'] <= 10:
            archive_data = {
                "original_link": orig['link'],
                "category": art['category'],
                "title": art['eng_title'],
                "summary": art['summary'],
                "image_url": img,
                "score": art['score'],
                "archive_reason": "Top 10 Rank"
            }
            supabase.table("search_archive").upsert(archive_data, on_conflict="original_link").execute()
            
        saved += 1
        if saved % 20 == 0: print(f"âœ… {saved}ê°œ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ...")

    print(f"ğŸ‰ ìµœì¢… ì™„ë£Œ: {saved}ê°œì˜ ì‹¤ì‹œê°„ ë‰´ìŠ¤ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run()
