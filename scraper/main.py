import sys
import os
import time
from datetime import datetime
from dotenv import load_dotenv

# ëª¨ë“ˆ import ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

# í•„ìˆ˜ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from scraper import crawler, ai_engine, repository
from scraper.config import CATEGORY_SEEDS, TOP_RANK_LIMIT

load_dotenv()

def run_master_scraper():
    print(f"ğŸš€ K-Enter Trend Master ê°€ë™ ì‹œì‘: {datetime.now()}")
    
    # ì„¤ì •ëœ 5ê°œ ì¹´í…Œê³ ë¦¬ ìˆœíšŒ
    for category, seeds in CATEGORY_SEEDS.items():
        print(f"\nğŸ“‚ [{category.upper()}] íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # ---------------------------------------------------------
        # [1ë‹¨ê³„] ì”¨ì•— ìˆ˜ì§‘ (Seed Search)
        # ---------------------------------------------------------
        seed_titles = []
        try:
            for seed in seeds:
                # display íŒŒë¼ë¯¸í„° ì‚¬ìš© (crawler.py ì—…ë°ì´íŠ¸ í•„ìˆ˜)
                news = crawler.get_naver_api_news(seed, display=20)
                seed_titles.extend([n['title'] for n in news])
            
            seed_titles = list(set(seed_titles)) # ì¤‘ë³µ ì œê±°
            print(f"   ğŸŒ± ì›ì„ ìˆ˜ì§‘ ì™„ë£Œ: {len(seed_titles)}ê°œì˜ ì œëª© í™•ë³´")
        except Exception as e:
            print(f"   âš ï¸ ì”¨ì•— ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
        
        # ---------------------------------------------------------
        # [2ë‹¨ê³„] í‚¤ì›Œë“œ ì¶”ì¶œ (AI Mining)
        # ---------------------------------------------------------
        top_keywords = ai_engine.extract_top_entities(category, seed_titles)
        
        if not top_keywords:
            print("   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™.")
            continue
            
        print(f"   ğŸ’ ì¶”ì¶œëœ ë­í‚¹(Top {len(top_keywords)}): {', '.join(top_keywords[:5])}...")

        # ---------------------------------------------------------
        # [3ë‹¨ê³„] í‚¤ì›Œë“œë³„ ì¢…í•© ìš”ì•½ (Deep Dive & Synthesis)
        # ---------------------------------------------------------
        category_news_list = []
        target_keywords = top_keywords[:TOP_RANK_LIMIT] # ì„¤ì •ëœ ê°œìˆ˜(30ê°œ)ë§Œí¼ ì²˜ë¦¬
        
        for rank, kw in enumerate(target_keywords):
            print(f"   ğŸ” Rank {rank+1}: '{kw}' ì¢…í•© ìš”ì•½ ì¤‘...")
            
            try:
                # 1. í•´ë‹¹ í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰
                raw_articles = crawler.get_naver_api_news(kw, display=10)
                if not raw_articles:
                    continue

                # 2. ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ìˆ˜ì§‘ (ì—¬ëŸ¬ ê¸°ì‚¬ í†µí•©)
                full_contents = []
                main_image = None
                
                for art in raw_articles[:5]: # ìƒìœ„ 5ê°œ ê¸°ì‚¬ë§Œ ì°¸ì¡°
                    text, img = crawler.get_article_data(art['link'])
                    if text: 
                        full_contents.append(text)
                    
                    # [ì´ë¯¸ì§€ ì²˜ë¦¬] ì²« ë²ˆì§¸ ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ë©”ì¸ìœ¼ë¡œ ì„¤ì •
                    if not main_image and img:
                        # http -> https ê°•ì œ ë³€í™˜ (ë³´ì•ˆ ì´ìŠˆ í•´ê²°)
                        if img.startswith("http://"):
                            img = img.replace("http://", "https://")
                        main_image = img

                # 3. ë°ì´í„° í¬ì¥ (ë‚´ìš©ì´ ìˆì„ ê²½ìš°ë§Œ)
                if full_contents:
                    # AIì—ê²Œ ì¢…í•© ë¸Œë¦¬í•‘ ì‘ì„± ìš”ì²­
                    briefing = ai_engine.synthesize_briefing(kw, full_contents)
                    
                    # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©
                    final_img = main_image or f"https://placehold.co/600x400/111/cyan?text={kw}"

                    news_item = {
                        "category": category,
                        "rank": rank + 1,
                        "keyword": kw,
                        "title": f"[{kw}] Key Trends & Issues", # ì œëª©ì€ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ í†µì¼
                        "summary": briefing,
                        "link": None,            # ğŸš¨ ìš”ì²­ì‚¬í•­: ê¸°ì‚¬ ë§í¬ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (NULL)
                        "image_url": final_img,  # ğŸš¨ ì´ë¯¸ì§€ëŠ” ì €ì¥í•¨ (HTTPS ì²˜ë¦¬ë¨)
                        "score": 10.0 - (rank * 0.1), # ìˆœìœ„ì— ë”°ë¥¸ ì ìˆ˜ ë¶€ì—¬
                        "likes": 0, 
                        "dislikes": 0,
                        "created_at": datetime.now().isoformat(),
                        "published_at": datetime.now().isoformat()
                    }
                    category_news_list.append(news_item)
                
                # API í˜¸ì¶œ ì†ë„ ì¡°ì ˆ (ì°¨ë‹¨ ë°©ì§€)
                time.sleep(0.5)
                
            except Exception as e:
                print(f"      âš ï¸ '{kw}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # ---------------------------------------------------------
        # [4ë‹¨ê³„] DB ì €ì¥ (Repository í˜¸ì¶œ)
        # ---------------------------------------------------------
        if category_news_list:
            # 1. ìƒìœ„ 10ê°œëŠ” ì•„ì¹´ì´ë¸Œ(ì—­ì‚¬)ì— ì €ì¥
            repository.save_to_archive(category_news_list[:10])
            
            # 2. Live News í…Œì´ë¸”ì€ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì „ì²´ êµì²´ (Refresh)
            repository.refresh_live_news(category, category_news_list)

    print("\nğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_master_scraper()
