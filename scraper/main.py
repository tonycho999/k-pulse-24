import os
import time
from datetime import datetime
from dotenv import load_dotenv

# ì‚¬ìš©ì ëª¨ë“ˆ ì„í¬íŠ¸
from config import CATEGORY_MAP
from scraper import crawler, ai_engine, repository, update_rankings

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def run_scraper():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ìš”ì•½ í•µì‹¬ ë¡œì§ (1íšŒ ì‹¤í–‰)"""
    print("ğŸš€ 7ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™...")
    
    for category, keywords in CATEGORY_MAP.items():
        try:
            print(f"\nğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘...")

            # 1. ìˆ˜ì§‘
            raw_news = []
            for kw in keywords: 
                raw_news.extend(crawler.get_naver_api_news(kw))
            
            # 2. ì¤‘ë³µ ì œê±°
            existing_links = repository.get_existing_links(category)
            
            new_candidate_news = []
            seen_links = set()
            for n in raw_news:
                if n['link'] not in existing_links and n['link'] not in seen_links:
                    new_candidate_news.append(n)
                    seen_links.add(n['link'])
            
            print(f"   ğŸ” ìˆ˜ì§‘: {len(raw_news)}ê°œ -> ê¸°ì¡´ DB ì¤‘ë³µ ì œì™¸: {len(new_candidate_news)}ê°œ")

            # ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ
            if not new_candidate_news:
                continue

            # 3. AI ì„ ë³„
            selected = ai_engine.ai_category_editor(category, new_candidate_news)
            print(f"   ã„´ AI ì„ ë³„ ì™„ë£Œ: {len(selected)}ê°œ")

            # 4. ì‹ ê·œ ë‰´ìŠ¤ ë°ì´í„° ìƒì„± ë° ì €ì¥
            if selected:
                new_data_list = []
                for i, art in enumerate(selected):
                    idx = art.get('original_index')
                    if idx is None or idx >= len(new_candidate_news): continue
                    
                    orig = new_candidate_news[idx]
                    img = crawler.get_article_image(orig['link']) or f"https://placehold.co/600x400/111/cyan?text={category}"

                    new_data_list.append({
                        "rank": art.get('rank', 99), 
                        "category": category, 
                        "title": art.get('eng_title', orig['title']),
                        "summary": art.get('summary', 'Detailed summary not available.'), 
                        "link": orig['link'], 
                        "image_url": img,
                        "score": art.get('score', 5.0), 
                        "likes": 0, 
                        "dislikes": 0, 
                        "created_at": datetime.now().isoformat(),
                        "published_at": orig.get('published_at', datetime.now()).isoformat()
                    })
                
                # DB ì €ì¥ (Repositoryì—ê²Œ ìœ„ì„)
                repository.save_news(new_data_list)

            # 5. ìŠ¬ë¡¯ ê´€ë¦¬ (30ê°œ ìœ ì§€)
            repository.manage_slots(category)

        except Exception as e:
            print(f"âš ï¸ Error processing category {category}: {e}")
            continue

    # [ë§ˆì§€ë§‰ ë‹¨ê³„] ì•„ì¹´ì´ë¹™ ë° í‚¤ì›Œë“œ ë¶„ì„ (ì„ íƒ ì‚¬í•­)
    try:
        print("\nğŸ“Š AI í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
        titles = repository.get_recent_titles()
        if titles:
            # ai_engineì— í•´ë‹¹ í•¨ìˆ˜ê°€ êµ¬í˜„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
            if hasattr(ai_engine, 'ai_analyze_keywords'):
                keywords = ai_engine.ai_analyze_keywords(titles)
                if keywords:
                    print(f"   ğŸ”¥ AI ì¶”ì¶œ íŠ¸ë Œë“œ: {[k.get('keyword') for k in keywords[:3]]}...")
                    repository.update_keywords_db(keywords)
            else:
                print("   â„¹ï¸ í‚¤ì›Œë“œ ë¶„ì„ í•¨ìˆ˜ê°€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒ¨ìŠ¤í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    
    print("ğŸ‰ ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ.")

def main():
    print("ğŸš€ K-Enter AI News Bot Started...")
    print(f"ğŸ•’ Time: {datetime.now()}")
    
    # [1] ìˆœìœ„ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ì‚¬ì´ë“œë°”ìš©)
    # ì•ˆì „ì¥ì¹˜ê°€ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ try-except ì—†ì´ í˜¸ì¶œí•´ë„ ë¨
    update_rankings.update_rankings() 
    
    # [2] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ìš”ì•½ ë¡œì§ ì‹¤í–‰
    run_scraper()
    
    print("âœ… All Tasks Completed Successfully. Exiting.")
    # GitHub ActionsëŠ” ì—¬ê¸°ì„œ ìŠ¤í¬ë¦½íŠ¸ê°€ ëë‚˜ë©´(Exit) ìë™ìœ¼ë¡œ 'ì„±ê³µ(Green)' ì²˜ë¦¬ë©ë‹ˆë‹¤.

if __name__ == "__main__":
    main()
