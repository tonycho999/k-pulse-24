import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import time
from datetime import datetime
from dotenv import load_dotenv

from scraper.config import CATEGORY_MAP
from scraper import crawler, ai_engine, repository, update_rankings

load_dotenv()

def run_scraper():
    print("ğŸš€ 7ë‹¨ê³„ ë§ˆìŠ¤í„° ì—”ì§„ ê°€ë™ (Rules 1-6 Applied)...")
    
    for category, keywords in CATEGORY_MAP.items():
        try:
            print(f"\nğŸ“‚ {category.upper()} ë¶€ë¬¸ ì²˜ë¦¬ ì¤‘...")

            # [ê·œì¹™ 1] ìˆ˜ì§‘ (ìµœì‹ ìˆœ ì •ë ¬ë¨)
            raw_news = []
            for kw in keywords: 
                raw_news.extend(crawler.get_naver_api_news(kw))
            
            # [ê·œì¹™ 2] ì¤‘ë³µ ì œê±°
            existing_links = repository.get_existing_links(category)
            
            new_candidate_news = []
            seen_links = set()
            for n in raw_news:
                if n['link'] not in existing_links and n['link'] not in seen_links:
                    new_candidate_news.append(n)
                    seen_links.add(n['link'])
            
            print(f"   ğŸ” ìˆ˜ì§‘: {len(raw_news)}ê°œ -> ì¤‘ë³µ ì œê±° í›„: {len(new_candidate_news)}ê°œ")

            if not new_candidate_news:
                continue

            # [ê·œì¹™ 3] ìµœì‹  ê¸°ì‚¬ 70ê°œ ì„ ì • -> AI í‰ê°€
            # 70ê°œê°€ ì•ˆ ë˜ë©´ ìˆëŠ” ë§Œí¼ë§Œ ë³´ëƒ„
            ai_input_news = new_candidate_news[:70]

            # AI ì„ ë³„ (ì ìˆ˜ ë¶€ì—¬ ë° ìš”ì•½)
            analyzed_list = ai_engine.ai_category_editor(category, ai_input_news)
            print(f"   ã„´ AI ë¶„ì„ ì™„ë£Œ: {len(analyzed_list)}ê°œ")

            if analyzed_list:
                # [ê·œì¹™ 3 í›„ë°˜] ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ 30ê°œ ì„ ì •
                # ì ìˆ˜(score) ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                analyzed_list.sort(key=lambda x: x.get('score', 0), reverse=True)
                
                # ìƒìœ„ 30ê°œë§Œ ìë¥´ê¸° (ê·œì¹™ 4: ìƒˆë¡œìš´ ê¸°ì‚¬ 30ê°œ ì €ì¥)
                top_30_news = analyzed_list[:30]
                
                new_data_list = []
                for art in top_30_news:
                    idx = art.get('original_index')
                    if idx is None or idx >= len(ai_input_news): continue
                    
                    orig = ai_input_news[idx]
                    img = crawler.get_article_image(orig['link']) or f"https://placehold.co/600x400/111/cyan?text={category}"

                    # Rank ì»¬ëŸ¼ ì œê±°ë¨
                    news_item = {
                        "category": category, 
                        "title": art.get('eng_title', orig['title']),
                        "summary": art.get('summary', 'Summary not available.'), 
                        "link": orig['link'], 
                        "image_url": img,
                        "score": art.get('score', 5.0), 
                        "likes": 0, 
                        "dislikes": 0, 
                        "created_at": datetime.now().isoformat(),
                        "published_at": orig.get('published_at', datetime.now()).isoformat()
                    }
                    new_data_list.append(news_item)
                
                # [ê·œì¹™ 4] DB ì €ì¥ (30ê°œ) + [ì•„ì¹´ì´ë¹™]
                repository.save_news(new_data_list)

            # [ê·œì¹™ 5 & 6] ìŠ¬ë¡¯ ê´€ë¦¬ (ì „ì²´ 30ê°œ ìœ ì§€, ì‹œê°„/ì ìˆ˜ ì‚­ì œ)
            repository.manage_slots(category)

        except Exception as e:
            print(f"âš ï¸ Error processing category {category}: {e}")
            continue

    # í‚¤ì›Œë“œ ë¶„ì„ (ì˜µì…˜)
    try:
        print("\nğŸ“Š AI í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
        titles = repository.get_recent_titles()
        if titles and hasattr(ai_engine, 'ai_analyze_keywords'):
            keywords = ai_engine.ai_analyze_keywords(titles)
            if keywords:
                repository.update_keywords_db(keywords)
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    print("ğŸ‰ ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ.")

def main():
    print("ğŸš€ K-Enter AI News Bot Started...")
    
    # ìˆœìœ„ ì—…ë°ì´íŠ¸
    update_rankings.update_rankings() 
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘
    run_scraper()
    
    print("âœ… All Tasks Completed.")

if __name__ == "__main__":
    main()
