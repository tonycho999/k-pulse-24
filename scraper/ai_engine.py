import os
import json
import time
import re
import requests
from groq import Groq
from scraper.config import CATEGORY_SEEDS # config ë³€ê²½ì‚¬í•­ ë°˜ì˜

# =========================================================
# 1. [í•µì‹¬] ì§€ëŠ¥í˜• ëª¨ë¸ í•„í„°ë§ (Text Generation Only)
# =========================================================

def get_groq_text_models():
    """
    [Groq] ì „ì²´ ëª¨ë¸ ì¤‘ 'Vision', 'Whisper' ë“± ê¸€ ëª» ì“°ëŠ” ëª¨ë¸ ì œì™¸
    """
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key: return []
    
    try:
        client = Groq(api_key=api_key)
        all_models = client.models.list()
        
        valid_models = []
        for m in all_models.data:
            mid = m.id.lower()
            # â›” ë¸”ë™ë¦¬ìŠ¤íŠ¸: ì´ë¯¸ì§€(vision), ìŒì„±(whisper, audio) ëª¨ë¸ ì œì™¸
            if 'vision' in mid or 'whisper' in mid or 'audio' in mid:
                continue
            valid_models.append(m.id)
            
        # ìµœì‹  ëª¨ë¸ì´ ìœ„ë¡œ ì˜¤ë„ë¡ ì—­ìˆœ ì •ë ¬ (Llama-3.3 > 3.1)
        valid_models.sort(reverse=True)
        # print(f"      âœ… Groq í…ìŠ¤íŠ¸ ëª¨ë¸ ì„ ë³„ ì™„ë£Œ: {len(valid_models)}ê°œ")
        return valid_models
    except Exception as e:
        print(f"      âš ï¸ Groq ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

def get_openrouter_text_models():
    """
    [OpenRouter] ì „ì²´ ì¤‘ 'Instruct', 'Chat'ë§Œ í¬í•¨í•˜ê³  'Diffusion' ë“± ì œì™¸
    """
    try:
        res = requests.get("https://openrouter.ai/api/v1/models", timeout=5)
        if res.status_code != 200: return []
        
        data = res.json().get('data', [])
        valid_models = []
        
        for m in data:
            mid = m['id'].lower()
            
            # âœ… í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸: ë¬´ë£Œ(:free)ì´ë©´ì„œ ëŒ€í™”í˜•(chat, instruct, gpt)ì¸ ê²ƒ
            if ':free' in mid and ('chat' in mid or 'instruct' in mid or 'gpt' in mid):
                # â›” ë¸”ë™ë¦¬ìŠ¤íŠ¸: ê·¸ë¦¼ ê·¸ë¦¬ëŠ” ëª¨ë¸(diffusion, image, vision) ì² ì €íˆ ë°°ì œ
                if 'diffusion' in mid or 'image' in mid or 'vision' in mid or '3d' in mid:
                    continue
                valid_models.append(m['id'])
        
        valid_models.sort(reverse=True)
        # print(f"      âœ… OpenRouter í…ìŠ¤íŠ¸ ëª¨ë¸ ì„ ë³„ ì™„ë£Œ: {len(valid_models)}ê°œ")
        return valid_models
    except Exception as e:
        # print(f"      âš ï¸ OpenRouter ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

def get_hf_text_models():
    """
    [Hugging Face] API ìì²´ í•„í„°ë§ ê¸°ëŠ¥ ì‚¬ìš© (pipeline_tag=text-generation)
    """
    try:
        # 'text-generation' íƒœê·¸ê°€ ë‹¬ë¦° ëª¨ë¸ë§Œ ìƒìœ„ 5ê°œ ê°€ì ¸ì˜¤ê¸°
        url = "https://huggingface.co/api/models?pipeline_tag=text-generation&sort=downloads&direction=-1&limit=5"
        res = requests.get(url, timeout=5)
        
        if res.status_code == 200:
            return [m['modelId'] for m in res.json()]
    except:
        pass
    return ["mistralai/Mistral-7B-Instruct-v0.3"] # ì‹¤íŒ¨ ì‹œ ì•ˆì „ë¹µ ëª¨ë¸

# =========================================================
# 2. ë§ˆìŠ¤í„° AI ì‹¤í–‰ ì—”ì§„ (ìˆœì°¨ì  ì‹œë„)
# =========================================================

def ask_ai_master(system_prompt, user_input):
    """
    Groq -> OpenRouter -> HF ìˆœì„œë¡œ 'í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸'ë§Œ ê³¨ë¼ì„œ ì‹œë„
    """
    
    # 1. Groq ì‹œë„
    groq_key = os.getenv("GROQ_API_KEY")
    if groq_key:
        models = get_groq_text_models()
        client = Groq(api_key=groq_key)
        
        for model_id in models:
            try:
                # print(f"      ğŸ¤– Groq ì‹œë„: {model_id}")
                completion = client.chat.completions.create(
                    model=model_id,
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_input}],
                    temperature=0.3
                )
                return completion.choices[0].message.content.strip()
            except Exception:
                continue

    # 2. OpenRouter ì‹œë„ (Groq ì‹¤íŒ¨ ì‹œ)
    or_key = os.getenv("OPENROUTER_API_KEY")
    if or_key:
        print("      ğŸš¨ Groq ì‹¤íŒ¨ -> OpenRouter ê°€ë™")
        models = get_openrouter_text_models()
        
        for model_id in models:
            try:
                # print(f"      ğŸ¤– OR ì‹œë„: {model_id}")
                res = requests.post(
                    url="https://openrouter.ai/api/v1/chat/completions",
                    headers={"Authorization": f"Bearer {or_key}"},
                    json={
                        "model": model_id,
                        "messages": [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_input}],
                        "temperature": 0.3
                    },
                    timeout=20
                )
                if res.status_code == 200:
                    content = res.json()['choices'][0]['message']['content']
                    if content: return content
            except:
                continue

    # 3. Hugging Face ì‹œë„ (ìµœí›„ì˜ ë³´ë£¨)
    hf_token = os.getenv("HF_API_TOKEN")
    if hf_token:
        print("      ğŸ’€ OpenRouter ì‹¤íŒ¨ -> HF ê°€ë™")
        models = get_hf_text_models()
        
        for model_id in models:
            try:
                API_URL = f"https://api-inference.huggingface.co/models/{model_id}"
                headers = {"Authorization": f"Bearer {hf_token}"}
                payload = {"inputs": f"<s>[INST] {system_prompt}\n\n{user_input} [/INST]"}
                res = requests.post(API_URL, headers=headers, json=payload, timeout=20)
                
                if res.status_code == 200:
                    data = res.json()
                    if isinstance(data, list) and 'generated_text' in data[0]:
                        return data[0]['generated_text']
                    elif isinstance(data, dict) and 'generated_text' in data:
                        return data['generated_text']
            except: continue

    return ""

# =========================================================
# 3. ê°•ë ¥í•œ JSON íŒŒì„œ (AI ì‚¬ì¡± ì œê±°ê¸°)
# =========================================================

def parse_json_result(text):
    """
    AIê°€ "Here is the JSON:" ê°™ì€ ë§ì„ ë¶™ì—¬ë„ ë¬´ì¡°ê±´ ìˆœìˆ˜ JSONë§Œ ì¶”ì¶œ
    """
    if not text: return []
    
    # 1. ê°€ì¥ ê¹”ë”í•œ ê²½ìš°
    try: return json.loads(text)
    except: pass
    
    # 2. ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ëŸ­ (```json) ì œê±°
    try:
        if "```" in text:
            # ```json ë’¤ì— ìˆëŠ” ë‚´ìš© ì¶”ì¶œ
            text = text.split("```json")[-1].split("```")[0].strip()
            # ë§Œì•½ json ì•ˆì“°ê³  ê·¸ëƒ¥ ``` ë§Œ ì¼ì„ ê²½ìš° ëŒ€ë¹„
            if not text.startswith("[") and not text.startswith("{"):
                 text = text.split("```")[-1].split("```")[0].strip()
            return json.loads(text)
    except: pass
    
    # 3. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ [ ... ] ë˜ëŠ” { ... } íŒ¨í„´ ê°•ì œ ì¶”ì¶œ (ê°€ì¥ ê°•ë ¥)
    try:
        match = re.search(r'(\[.*\]|\{.*\})', text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
    except: pass
    
    return []

# =========================================================
# 4. ì™¸ë¶€ í˜¸ì¶œ ì¸í„°í˜ì´ìŠ¤ (ê¸°ì¡´ ë¡œì§ ìœ ì§€ ë° ì‹ ê·œ ì¶”ê°€)
# =========================================================

def extract_top_entities(category, news_titles):
    """
    [New] ì œëª© ë­‰ì¹˜ì—ì„œ ê°€ìˆ˜, ë°°ìš°, ì‘í’ˆëª… ë“± ê³ ìœ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ ë­í‚¹ ì„ ì •
    """
    system_prompt = f"""
    You are a Trend Analyst for '{category}'. 
    Extract the most mentioned entities (Singers, Actors, Titles, Brands, Places) from the provided news titles.
    Rules:
    1. Identify specific names (e.g., "BTS", "Kim Soo-hyun", "Squid Game 2").
    2. Rank them by frequency of appearance.
    3. Return a JSON ARRAY of strings only. Maximum 30 items.
    4. Translate Korean names to English if possible, or keep Korean.
    Example Output: ["NewJeans", "IU", "Hype Boy", "G-Dragon"]
    """
    
    # ì œëª©ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•© (ë„ˆë¬´ ê¸¸ë©´ ìë¦„)
    user_input = "\n".join(news_titles)[:10000]
    raw_result = ask_ai_master(system_prompt, user_input)
    
    parsed = parse_json_result(raw_result)
    return parsed if isinstance(parsed, list) else []

def synthesize_briefing(keyword, news_contents):
    """
    [New] íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ì—¬ëŸ¬ ê¸°ì‚¬ ë‚´ìš©ì„ í•˜ë‚˜ë¡œ í•©ì³ ë¸Œë¦¬í•‘ ìƒì„±
    """
    system_prompt = f"""
    You are a Professional News Briefing Editor. 
    Topic: {keyword}
    
    Task:
    Summarize the provided news snippets into a 5-10 line cohesive briefing in English.
    Focus on: What is happening, Why it is trending, and Public reaction.
    
    [Format]
    - Style: Professional, Engaging, Journalistic
    - Length: 5 to 10 lines
    - Output: Plain text only (No Markdown, No JSON)
    """
    
    # ë‚´ìš© ê²°í•© (í† í° ì œí•œ ê³ ë ¤)
    user_input = "\n\n".join(news_contents)[:4000] 
    briefing = ask_ai_master(system_prompt, user_input)
    return briefing
